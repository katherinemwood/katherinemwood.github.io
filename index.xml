<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Inattentional Coffee on Inattentional Coffee</title>
    <link>https://katherinemwood.github.io/</link>
    <description>Recent content in Inattentional Coffee on Inattentional Coffee</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Katherine</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>5 Minute Simulation: Variation in effect size estimates</title>
      <link>https://katherinemwood.github.io/post/es_var/</link>
      <pubDate>Mon, 26 Jun 2017 13:54:20 -0500</pubDate>
      
      <guid>https://katherinemwood.github.io/post/es_var/</guid>
      <description>&lt;p&gt;If it’s not terribly obvious, &lt;a href=&#34;http://shiny.rstudio.com/tutorial/&#34;&gt;Shiny&lt;/a&gt; is my new favorite toy. It’s incredibly accessible for beginners, gives you great results with minimal effort, and can be as sophisticated as you need it to be.&lt;/p&gt;
&lt;p&gt;I decided to throw together a quick simulation to look at the variation in effect size estimates we can expect at different sample sizes. There’s an increasing focus in psych on estimation of effects, rather than simply detection of effects. This is great, but, as it turns out, virtually impossible with a single study unless you are prepared to recruit massive numbers of subjects. Nothing here is new, but I like looking at distributions and playing with sliders, and I’ll take any excuse to make a little shiny widget.&lt;/p&gt;
&lt;p&gt;In this simulation, we’re doing a basic, between-groups t-test and drawing samples from the normal distribution. The code can be dead simple. I’ll write a tiny&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)

sim_es &amp;lt;- function(n, true_es) {
  g1 &amp;lt;- rnorm(n, true_es, 1)
  g2 &amp;lt;- rnorm(n, 0, 1)
  return(effsize::cohen.d(g1, g2, paired=FALSE)$estimate)
}

plot_es &amp;lt;- function(data, true_es) {
  es_plot &amp;lt;- ggplot() +
      theme_minimal() +
      geom_histogram(aes(x=data, y=..count../sum(..count..)), 
               color=&amp;#39;darkblue&amp;#39;, fill=&amp;#39;darkblue&amp;#39;, position=&amp;#39;identity&amp;#39;) +
      geom_vline(xintercept=c(true_es,
                              sort(data)[.975*length(data)], 
                              sort(data)[.025*length(data)]), 
                 size=1.5, color=c(&amp;#39;lightgray&amp;#39;, &amp;#39;red&amp;#39;, &amp;#39;red&amp;#39;), 
                 linetype=c(&amp;#39;solid&amp;#39;, &amp;#39;longdash&amp;#39;,&amp;#39;longdash&amp;#39;)) +
      xlab(&amp;quot;Effect size&amp;quot;) +
      ylab(&amp;#39;Proportion&amp;#39;) +
      xlim(c(-2, 2))

  print(es_plot)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s what you get if you use tiny (n=10) groups (perhaps under the justification that if an effect is big, you’ll detect it just fine in small samples) and no true effect is present:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n10 &amp;lt;- replicate(3000, sim_es(10, 0))
plot_es(n10, 0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://katherinemwood.github.io/post/es_var_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The white line is the true effect, and the two red lines mark out the 95% quantile.&lt;/p&gt;
&lt;p&gt;Yikes. With samples that small, you could (and will, often!) get an enormous effect when none is present.&lt;/p&gt;
&lt;p&gt;Here’s what we get with n = 50, no effect present. I’ve left the x-axis fixed to make it easier to compare all of these plots.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n50 &amp;lt;- replicate(3000, sim_es(50, 0))
plot_es(n50, 0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://katherinemwood.github.io/post/es_var_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is a dramatic improvement over n=10, but you could still estimate what is considered a small (d=.1, traditionally) to medium (d=.3, traditionally) effect in either direction with appreciable frequency.&lt;/p&gt;
&lt;p&gt;And here’s n=200.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n200 &amp;lt;- replicate(3000, sim_es(200, 0))
plot_es(n200, 0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://katherinemwood.github.io/post/es_var_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I’ve used d=0 as an example, but you get this spread regardless of what the true d is; it will just shift to center on the true effect. In that case, you’ll detect an effect most of the time, but can be way off about its actual size. This doesn’t mean that you can throw power out the window by arguing that you only care about detection, of course–you’ll “detect” an effect a lot of the time when d=0 with small samples.&lt;/p&gt;
&lt;p&gt;These simulations are the result of 3000 replications each, but in the shiny app you can change this number.&lt;/p&gt;
&lt;p&gt;For me, this really drives home just how important replications and meta-analyses–cumulative science in general, really–are, particularly for estimation. When you do a lot of these studies over and over again, as these simulations model, you’ll zero in on the true effect, but a study can’t do it alone.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://katherinemwood.shinyapps.io/effect_size/&#34;&gt;The Shiny app can be found here&lt;/a&gt;. You can tweak group size, true effect size, how many simulations are run, and the limits on the x-axis. You can also view a plot of the corresponding p-values.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/katherinemwood/es_variation&#34;&gt;The source code can be found here.&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Open Science in Practice</title>
      <link>https://katherinemwood.github.io/post/open_sci/</link>
      <pubDate>Mon, 26 Jun 2017 13:48:35 -0500</pubDate>
      
      <guid>https://katherinemwood.github.io/post/open_sci/</guid>
      <description>&lt;p&gt;I will finish grad school a child of the replication crisis. My year and a half in grad school has been marked by bombshell failures-to-replicate, increasing adoption of Bayesian methods, and wider recognition of the insidious effects of questionable research practices.&lt;/p&gt;
&lt;p&gt;In many ways, I’m lucky. I didn’t have to make changes to the way I did things to adhere to better practices. I got to start with openness as one of the core tenets of how I conduct my research, and starting fresh is always easier than having to alter pre-existing patterns of behavior. It also helps to have a huge proponent of replication and open science as an advisor and to have come in at a time when there’s fantastic infrastructure in place.&lt;/p&gt;
&lt;p&gt;Here’s an example of what my process looks like. A lot of this has already been said by better minds, and none of what I do is revolutionary. But it can, perhaps, serve as an illustration of what it looks like to actually build these practices into your workflow.&lt;/p&gt;
&lt;div id=&#34;planning-and-preregistration&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Planning and preregistration&lt;/h2&gt;
&lt;p&gt;Once my advisor and I have finalized the approach to our next experiment, the preparation phase begins. I write-up and test my experiment script by running through the entire thing, as a subject would (and try to break it as a subject might).&lt;/p&gt;
&lt;p&gt;Because I’ve written the experiment script, I know exactly how the data will be saved out. This lets me write the entire analysis script, including wrangling, pre-processing, and figure generation. I test the functionality of the script on dummy data I generate during testing my experiment and make sure it runs properly.&lt;/p&gt;
&lt;p&gt;After this is done, it’s time to preregister–I like to do so at the &lt;a href=&#34;https://osf.io/&#34;&gt;Open Science Framework&lt;/a&gt;. I outline my hypotheses and upload hypothetical data figures if my predictions are sufficiently specific. I describe all of my experiment methods in gory detail, including all measures collected, and attach my experimental script and any stimuli I use. I detail exactly which hypotheses I’m going to test, and how (which statistical test; how many tails, if applicable; the values of any parameters, such as priors for Bayes Factors, etc.). I explicitly state how many subjects we’ll be running and how we’re going to exclude subjects from analysis. I also upload my analysis script and any helper functions or files.&lt;/p&gt;
&lt;p&gt;Once all of these things are in place, I preregister the project. My preferred method is to enter it into embargo, and then make the project public after the paper has been submitted.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-collection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Data collection&lt;/h2&gt;
&lt;p&gt;This tends to be uneventful, since everything has been battle-tested and all decision have already been made. I do not examine the data, even just to look, as it comes in, so that if something unforeseen emerges that requires a change to protocol, I can make a new preregistration on OSF and affirm that it has been done in absence of knowledge of the data. I collect as many subjects as I said I would in my preregistration.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Analysis&lt;/h2&gt;
&lt;p&gt;This also goes lightning-fast, because the script is already written. I just run the data through it, and know the results that I will be reporting as confirmatory immediately (no strolls through the garden of forking paths). This is great for me, because I cannot handle suspense when it comes to knowing the outcome of my experiments.&lt;/p&gt;
&lt;p&gt;That done, I can then poke around a little and explore the data if I wish, or do some robustness analyses. Anything that comes from this needs to be reported as exploratory if it is reported at all.&lt;/p&gt;
&lt;p&gt;I make sure to plot my raw data. Ideally I like to plot it along with my summary values in something like a violin plot (I like to do the raw points over the violin density graph, plus a line showing the mean and maybe some confidence intervals). It’s always a good idea to know the ins and outs of your data, which requires knowing what the raw distributions look like.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;post-raw-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4. Post raw data&lt;/h2&gt;
&lt;p&gt;After the analysis is done, I add the raw data in its entirety to the project’s OSF page and include with it a README that provides detailed instructions on how to interpret the data. This has the nice added benefit of creating a secure backup of my data then and there.&lt;/p&gt;
&lt;p&gt;The project is still private at this point.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reporting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;5. Reporting&lt;/h2&gt;
&lt;p&gt;After repeating steps 1-4 as necessary for follow-up experiments, I write up the manuscript. The methods section is easy, as I pretty much wrote it already back in step 1. I make sure to include a link to my OSF page for the project, which I make public when submitting the manuscript, and include language making it clear that we report all conditions and measures.&lt;/p&gt;
&lt;p&gt;When we report our stats, we include all information necessary to recreate our analyses. This means correlations for within-subjects designs and cell means for ANOVAs. We also make sure to report how many people were excluded and what proportion of our sample that represents, and how many people ended up assigned to each condition, if applicable.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pre-prints&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;6. Pre-prints&lt;/h2&gt;
&lt;p&gt;After the manuscript has been accepted (I’ll take Wishful Thinking for $800, Alex), I upload a preprint of the accepted version to PsyarXiv. I will do this for all of my papers, but I think it’s especially important in cases where one cannot publish open access. Most journals will tell you what the policy is regarding posting the paper to external sources.&lt;/p&gt;
&lt;p&gt;You can, of course, post a preprint before the paper has been accepted (most of the time this will not preclude later publishing in a journal), but my papers tend to undergo radical transformation during peer review, so I prefer to post a more final version.&lt;/p&gt;
&lt;p&gt;And that’s my process. Integrating open science and best research practices takes a little practice, but once you’ve gotten comfortable with your workflow it’s as easy as any other habit (and comes with a lot of benefits for your trouble).&lt;/p&gt;
&lt;p&gt;This workflow works pretty well for me, but I think it can be better. This year I would like to start using figshare, and do version control for my scripts with GitHub, and look into using RMarkdown for papers. There’s always more to learn!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>5 Minute Simulation: Lonely Cards</title>
      <link>https://katherinemwood.github.io/post/lonely_cards/</link>
      <pubDate>Mon, 26 Jun 2017 12:34:18 -0500</pubDate>
      
      <guid>https://katherinemwood.github.io/post/lonely_cards/</guid>
      <description>&lt;p&gt;FiveThirtyEight posts math, logic, and probability puzzles each week. I was tempted by this week’s &lt;a href=&#34;http://fivethirtyeight.com/features/can-you-deal-with-these-card-game-puzzles/&#34;&gt;Riddler Express&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;On snowy afternoons, you like to play a solitaire “game” with a standard, randomly shuffled deck of 52 cards. You start dealing cards face up, one at a time, into a pile. As you deal each card, you also speak aloud, in order, the 13 card faces in a standard deck: ace, two, three, etc. (When you get to king, you start over at ace.) You keep doing this until the rank of the card you deal matches the rank you speak aloud, in which case you lose. You win if you reach the end of the deck without any matches.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;What is the probability that you win?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;My combinatorics skills are rusty, so I’m still hacking away at the closed-form solution. But, where cunning fails, brute force may suffice. While the closed-form approach is proving challenging (for me), this is fairly trivial to approximate through simulation. Recreating the game itself only takes a line or two of code, and then we can “play” hundreds of thousands of simulated games to get an idea of how often we win. &lt;a href=&#34;https://katherinemwood.shinyapps.io/lonely_cards/&#34;&gt;Here’s a simple shiny app&lt;/a&gt; with the results, and here’s the dead-simple code for simulating the game itself:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cards &amp;lt;- function() {
#make a deck; leave this sequence fixed for the verbal sequence,
#and scramble it to simulate the cards being dealt
deck &amp;lt;- rep(c(&amp;#39;ace&amp;#39;, as.character(seq(2:10)),
&amp;#39;jack&amp;#39;, &amp;#39;queen&amp;#39;, &amp;#39;king&amp;#39;), times=4)
#You win if none of the ranks line up; you lose if one or more does
return(as.numeric(!(sum(deck == sample(deck)) &amp;gt; 0))) #1 is win, 0 is lose
}
results &amp;lt;- replicate(100000, cards())
paste(&amp;#39;Win percentage: &amp;#39;, sum(results)/1000,&amp;#39;%&amp;#39;, sep=&amp;#39;&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Win percentage: 1.67%&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I’ll update this post with the closed-form solution if I manage to work it out (failing that, I’ll just link to the solution on FiveThirtyEight).&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/katherinemwood/lonely_cards&#34;&gt;Here’s the source code&lt;/a&gt; for the shiny app.&lt;/p&gt;
&lt;p&gt;UPDATE: It turns out the &lt;a href=&#34;https://fivethirtyeight.com/features/how-long-will-it-take-to-blow-out-the-birthday-candles/#correction&#34;&gt;closed-form solution to this riddle&lt;/a&gt; is nontrivial! It relies on a branch of combinatorics called &lt;a href=&#34;http://mathworld.wolfram.com/Derangement.html&#34;&gt;“derangements”&lt;/a&gt; I didn’t encounter in my (admittedly less than comprehensive) math and stats education.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Reconciling change blindness with long-term memory for objects</title>
      <link>https://katherinemwood.github.io/publication/cbltm/</link>
      <pubDate>Wed, 23 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>https://katherinemwood.github.io/publication/cbltm/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Shiny Effect Sizes</title>
      <link>https://katherinemwood.github.io/project/shiny_es/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://katherinemwood.github.io/project/shiny_es/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Two Sisters, One Dark Sky</title>
      <link>https://katherinemwood.github.io/project/darksky/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://katherinemwood.github.io/project/darksky/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
