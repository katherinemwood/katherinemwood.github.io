<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Inattentional Coffee</title>
    <link>https://katherinemwood.github.io/post/</link>
    <description>Recent content in Posts on Inattentional Coffee</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Katherine</copyright>
    <lastBuildDate>Sun, 01 Jan 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/post/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>CORVIDS 2.0: Faster, more stable, and now on Windows</title>
      <link>https://katherinemwood.github.io/post/corvids2.0/</link>
      <pubDate>Tue, 08 May 2018 10:04:53 -0500</pubDate>
      
      <guid>https://katherinemwood.github.io/post/corvids2.0/</guid>
      <description>&lt;p&gt;Some exciting CORVIDS news–we made some &lt;em&gt;major&lt;/em&gt; improvements to how things work, and we’ve released a brand new version! We now also have a standalone for Windows users, so no more wrestling with dependencies.&lt;/p&gt;
&lt;p&gt;If you want details on what’s changed, read on. If you just want the good stuff, head over to the &lt;a href=&#34;https://github.com/katherinemwood/corvids/releases/tag/v2.0.0&#34;&gt;latest release.&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;whats-new&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What’s new:&lt;/h1&gt;
&lt;div id=&#34;speeeeeeeed&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;SPEEEEEEEED&lt;/h2&gt;
&lt;p&gt;We used some mathematical properties of variances to vastly speed up solving time a factor of 2n^2, where n is the number of subjects/samples (details are in the &lt;a href=&#34;https://osf.io/rvgqk/&#34;&gt;updated preprint&lt;/a&gt; for the curious). Now the program will only check at valid variances, and it won’t waste any time checking variances that are impossible. This cuts the number of pairs it has to search way down, and you no longer have to be gun-shy about how much tolerance you set. Even large tolerances will only add a constrained number of possible variances (dependent on scale and sample size).&lt;/p&gt;
&lt;p&gt;Here’s a comparison of the old CORVIDS searching with large tolerance vs. the new CORVIDS, with a scale of 1-7 and a sample size of 10.&lt;/p&gt;
&lt;p&gt;Before:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;../img/corvids1.0_small_scale.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;That’s a &lt;em&gt;lot&lt;/em&gt; of combinations to search through (it is too many). CORVIDS would dismiss bad pairs pretty quickly, but it all added up to a lot of waiting time.&lt;/p&gt;
&lt;p&gt;After:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;../img/corvids2.0_small_scale.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;That’s a lot fewer combinations, and it translates to way more speed. It also works at scales and sample sizes of any magnitude, so you no longer have to be nervous about adding tolerance to big scales and/or samples.&lt;/p&gt;
&lt;p&gt;Old CORVIDS starts a long journey for n=50 and scale of 1-10:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;../img/corvids1.0_big_scale.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Yikes. That’s, shall we say, less than ideal.&lt;/p&gt;
&lt;p&gt;But new CORVIDS cuts right to the chase:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;../img/corvids2.0_big_scale.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Much better.&lt;/p&gt;
&lt;p&gt;We’re thrilled we were able to speed things up so much, and we hope it improves your experience using the program.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;find-first&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Find First&lt;/h2&gt;
&lt;p&gt;If you’re only interested in whether numbers are possible and don’t care about getting the full solution space, we have a &lt;code&gt;Stop After First Solution&lt;/code&gt; option. We revamped how it works, making it faster. It also now returns solutions at multiple means/variances, if they exist.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;windows-standalone&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Windows standalone&lt;/h2&gt;
&lt;p&gt;CORVIDS has a handful of dependencies that can be difficult to get working in harmony. Mac users always had the option of the standalone app, and now Windows users do too. Thanks for your patience, Windows users!&lt;/p&gt;
&lt;p&gt;We recommend using the standalone apps for routine applications. Unless you’re doing something ambitious or need programmatic access to the tool, the standalone app will give you all of the functionality with none of the pain. If you’ve struggled to get things working, we hope the standalone builds solve your problems.&lt;/p&gt;
&lt;p&gt;We also did some minor UI adjustments and bug fixes, some of them fairly substantial. You can view the changelog on the release page.&lt;/p&gt;
&lt;p&gt;If you notice any bugs or problems, feel free to email us or submit an issue on Github. Thanks to James Heathers, Nick Brown, and Jordan Anaya for their feedback and input as we’ve been making improvements.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>CORVIDS: A provably-complete data reconstruction tool</title>
      <link>https://katherinemwood.github.io/post/corvids/</link>
      <pubDate>Mon, 29 Jan 2018 12:36:28 -0600</pubDate>
      
      <guid>https://katherinemwood.github.io/post/corvids/</guid>
      <description>&lt;p&gt;This is a very exciting post for me, as it marks the release of a project that I have been working on with my peerless colleague and dear friend &lt;a href=&#34;http://www.informatics.illinois.edu/people-2/phd-students/#sean&#34;&gt;Sean Wilner&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This project started with a question from my advisor–did I think change-making algorithms could be used to reconstruct Likert scale data?–and ended with a complete data-reconstruction system that takes in a few summary statistics for Likert-scale-type data and returns every possible data set which could have generated those statistics, or informs you if no such data exist.&lt;/p&gt;
&lt;p&gt;The details of the implementation can be found in our &lt;a href=&#34;https://psyarxiv.com/7shr8/&#34;&gt;pre-print&lt;/a&gt;, and the code is open source and available in a handful of formats &lt;a href=&#34;https://github.com/katherinemwood/corvids/releases/tag/v1.0.0&#34;&gt;here&lt;/a&gt;. You can use the source code itself, or download a standalone compiled version of the app (recommended).&lt;/p&gt;
&lt;p&gt;Briefly sketched, the CORVIDS (COmplete Reconstruction of Values In a Diophantine System) relies on two things. One is the constrained nature of the data; we know that responses will fall between two endpoints (1 - 7, 0 - 10, etc.) and that the values must be integers. It turns out, these constraints, along with mean, variance (-or- standard deviation), and sample size, are sufficient to reconstruct the raw data. Using this information, we can set up a system of linear Diophantine equations (linear equations which can only take integer values) to solve for all possible data sets. In order to do this, we set up three equations–one for the mean, one for the variance, and one for the sample size–and solve the resulting system of equations. The result is a single solution and a set of transformations which describe all possible solutions to that system of equations (including the &lt;strong&gt;actual&lt;/strong&gt; raw data!).&lt;/p&gt;
&lt;p&gt;The theoretical side is pure, provable math, but extra steps need to be taken to make sure something like this can work on real (that is, messy) data. The biggest consideration is precision and rounding. Statistics aren’t always reported to sufficient precision, and rounding errors happen. How do we ensure that we can account for this and still find solutions to reasonable values? We build tolerances into the process. To that end, we have precision arguments you can set that will put an envelope around the mean and variance. The program then finds and chugs through all possible valid combinations of means and variances, looking for viable solutions to all of them. Usually there’s only one valid mean-variance pair in the neighborhood that has a viable solution, but sometimes you will end up with a few candidates that have valid solutions. But this isn’t a problem since the program will give you all of them.&lt;/p&gt;
&lt;p&gt;You can also play around with what happens if you forbid certain values from appearing in a solution (“can you still get these numbers if no one is allowed to respond with 1 or 7?”), or if you force certain values to be present (“can you still get solutions if at least 3 people responded 4?”).&lt;/p&gt;
&lt;p&gt;A nice test case emerged recently as detailed by Nick Brown and James Heathers, the first installment of which is described &lt;a href=&#34;https://medium.com/@jamesheathers/long-hair-dont-care-5eeba266ec52&#34;&gt;here&lt;/a&gt;. Here we have data on an integer scale (1-3) with means, standard deviations, and sample sizes reported for every condition.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;../img/hair_data.png&#34; title=&#34;Summary statistics from the paper&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;This is enough for CORVIDS to work. What’s also nice in this instance is that Nick Brown and James Heathers already reconstructed the data manually, &lt;strong&gt;and&lt;/strong&gt; the researcher shared their raw data. So we have ground truth to compare it against. With such a small scale, even at relatively low precision, I was able to reconstruct data for all 6 conditions in about 2 minutes.&lt;/p&gt;
&lt;p&gt;Here’s the process: For the first condition (male pedestrians, loose-haired confederate) This is what we would enter into CORVIDS:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;../img/corvids_interface.png&#34; title=&#34;Data entry fields&#34; style=&#34;width:50.0%&#34; style=&#34;height:50.0%&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;As you can see, we’re going to set a fairly generous tolerance for both the mean and standard deviation to account for rounding.&lt;/p&gt;
&lt;p&gt;Once our values are entered, we just hit “Start!” CORVIDS will now tell us how many mean/standard deviation combinations it’s going to search through (kind of a lot in this case, when precision is low).&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;../img/corvids_startup.png&#34; title=&#34;Starting message&#34; style=&#34;width:50.0%&#34; style=&#34;height:50.0%&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Then all we do is wait. Because the scale is so constrained (just 1 - 3), this process is going to be very fast. For larger scales and large numbers of subjects, it can take much longer.&lt;/p&gt;
&lt;p&gt;Here’s what CORVIDS will return once it’s done:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;../img/corvids_finish.png&#34; title=&#34;After reconstruction&#34; style=&#34;width:50.0%&#34; style=&#34;height:50.0%&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;We can see in this case that there is one, and only one, solution. From here, if we wanted, we could save this data out, graph it, save our parameters or save the entire model (paramters and data) to save time later.&lt;/p&gt;
&lt;p&gt;This is a nice case, because a reconstruction already exists, and the original author shared his raw data. CORVIDS returns the correct answer.&lt;/p&gt;
&lt;p&gt;Just for illustrative purposes, here’s what happens if we &lt;em&gt;really&lt;/em&gt; relax the tolerance on the mean and standard deviation. We end up with more solutions, at different mean/standard deviation combinations:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;../img/corvids_tolerance_sol.png&#34; style=&#34;width:50.0%&#34; style=&#34;height:50.0%&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;CORVIDS can also graph the results, so we can see how these distributions differ from one another:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;../img/tolerance.png&#34; style=&#34;width:50.0%&#34; style=&#34;height:50.0%&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;One data point switches from 2 to 3 each time.&lt;/p&gt;
&lt;p&gt;We can mow through the rest of the conditions in short order, and get the raw data for the other six:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../img/corvids_mbun.png&#34; style=&#34;width:50.0%&#34; style=&#34;height:50.0%&#34; /&gt; &lt;img src=&#34;../img/corvids_fnat.png&#34; style=&#34;width:50.0%&#34; style=&#34;height:50.0%&#34; /&gt; &lt;img src=&#34;../img/corvids_fpony.png&#34; style=&#34;width:50.0%&#34; style=&#34;height:50.0%&#34; /&gt; &lt;img src=&#34;../img/corvids_fbun.png&#34; style=&#34;width:50.0%&#34; style=&#34;height:50.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If the data weren’t already known, we would know exactly how each individual subject responded.&lt;/p&gt;
&lt;p&gt;This is a straightforward case, because the data are so constrained that there happened to be only one solution for each condition. More often, scales are wider than 1-3, there might be more subjects, and often there will be multiple solutions. In that case, you can get a sense of the “family resemblance” between the data sets. Sometimes solutions will all look similar–they might all be right-skewed, for instance. Other times a great many distribution shapes will be possible.&lt;/p&gt;
&lt;p&gt;This method can also, in some lucky cases, help resurrect data that was destroyed or simply lost to time. If enough of the summary statistics survived, the raw data can be found again.&lt;/p&gt;
&lt;p&gt;We encourage you to download the code and try it yourself! You can submit any issues or bug reports on Github.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Improving a data visualization: Connecticut edition</title>
      <link>https://katherinemwood.github.io/post/vizfix/</link>
      <pubDate>Sat, 16 Dec 2017 15:11:42 -0600</pubDate>
      
      <guid>https://katherinemwood.github.io/post/vizfix/</guid>
      <description>&lt;p&gt;I came across a graph not long ago that I immediately saved for later use. Making clean, effective data visualizations is hard, and there is always someone waiting to gleefuly inform you that you’ve messed it up in one way or another. Sometimes these are minor aesthetic quibbles, and the visualization still manages to communicate what it needs to effectively.&lt;/p&gt;
&lt;p&gt;Other times, missteps make the graph harder to interpret, and put too much onus on the viewer to make sense of what they’re seeing.&lt;/p&gt;
&lt;p&gt;This was one of those latter cases.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;../img/cap_gains.jpg&#34; title=&#34;Connecticut Tax Report&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;There is a lot going on here. It’s extremely difficult to parse this correctly. We’ve got:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Two y-axes on two different scales, meaning that two radically different values show up as overlapping, and very similar values appear to be much farther apart than they ought to be&lt;/li&gt;
&lt;li&gt;Two y-axes and no way to tell which set of data goes with which axis without a legend&lt;/li&gt;
&lt;li&gt;The same data type (growth rate) represented by two separate y-axes scaled to different values&lt;/li&gt;
&lt;li&gt;In addition to the two axes, every data point is annotated in the graph itself (which should obviate the need for axes, or vice-versa)&lt;/li&gt;
&lt;li&gt;These labels have inconsistent positions, and sometimes both points are labeled, and sometimes only one is&lt;/li&gt;
&lt;li&gt;Data of the same type (value over time) represented by two different formats, a bar and a line, providing stark visual difference and therefore implying difference in the data itself even though none exists&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the defense of the graph and its makers, some of these choices make sense &lt;em&gt;in a way&lt;/em&gt;. The cap gains growth varies a lot more than the enconomic growth rate, which could make it harder to see the take-home pattern. The smaller fluctuations in the income tax growth rate could get swamped by the much larger swings in cap gains. I think, though, that in an effort to get around this problem (if that was the reason), more were introduced.&lt;/p&gt;
&lt;p&gt;This graph has a straightforward message it wants to communicate: cap gains and state income tax vary together. When one goes up, so does the other; when one falls, the other tumbles after. This is the message the graph should communicate. Let’s see if we can rework this a little bit.&lt;/p&gt;
&lt;p&gt;Since this is just an excersize, I’m not going to fret too much about recovering &lt;em&gt;exactly&lt;/em&gt; the right values. Most of them are helpfully labeled, but for the handful that aren’t I’ll just eyeball it from the graph.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ct &amp;lt;- data.frame(&amp;#39;type&amp;#39;=rep(c(&amp;#39;Capital Gains&amp;#39;, &amp;#39;Income Tax&amp;#39;), each=14),
                 &amp;#39;year&amp;#39;=rep(2001:2014, 2),
                 &amp;#39;growth_rate&amp;#39;=c(30, -52, -17, 40, 23, 30, 17, 32, -60,
                                 -40, 93, -10, 46, -9,
                                 19, -23.5, -14.7, 21.9, 22.8, 19.4, 13,
                                 17.9, -27.3, -21.3, 27.7, .3, 21.9, -.9))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s just see how things look if we do a straightforward connected line graph. On the y-axis will simply be the growth rate, on the x-axis, the fiscal year. Cap gains and income tax will be connected lines, colored accordingly.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)

gains &amp;lt;- ggplot(data=ct, aes(x=year, y=growth_rate, color=type)) +
  geom_point() +
  geom_line() +
  theme_minimal()

print(gains)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../post/vizfix_files/figure-html/rough_line-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That doesn’t look too bad for a first step! The trends are still very visible–you can still clearly see that at every data point cap gains tracks with income tax. The difference in scales doesn’t actually obscure the pattern, and you can still see the changes for income tax even though they are smaller.&lt;/p&gt;
&lt;p&gt;We’re not done yet, though. Let’s clean this up some more. We’ll need to fix the labels, put each year visible on the x-axis, make the y-axis intervals a little more granular, and add a line at 0 to make it obvious when things dip into the negative.&lt;/p&gt;
&lt;p&gt;I’ll reproduce the entire plotting syntax, for clarity.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gains_tweaks &amp;lt;- ggplot(data=ct, aes(x=year, y=growth_rate, color=type)) +
  geom_hline(yintercept=0, color=&amp;#39;darkgray&amp;#39;) +
  geom_line() +
  geom_point() +
  labs(title = &amp;quot;When Investments Falter, So Does Connecticut&amp;quot;,
       subtitle = &amp;quot;Annual Percentage Change in State Income Tax vs. Capital Gains&amp;quot;,
       caption = &amp;quot;Note: Capital Gains are for the immediately preceding calendar year.&amp;quot;,
       y = &amp;quot;Growth Rate (%)&amp;quot;,
       x = &amp;quot;Fiscal Year&amp;quot;,
       color=NULL) +
  scale_y_continuous(breaks=seq(-60,100, by=20)) +
  scale_x_continuous(breaks=seq(2001, 2014, by=2)) +
  theme_minimal() +
  theme(panel.grid.minor = element_blank(),
        plot.caption = element_text(hjust=0, size=10),
        plot.title = element_text(margin=margin(0, 0, 5, 0)),
        plot.subtitle = element_text(margin=margin(0, 0, 10, 0)),
        axis.title.x = element_text(margin=margin(10, 0, 10, 0)))

print(gains_tweaks)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../post/vizfix_files/figure-html/clean_gains-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That’s an improvement, I’d say. One could quibble about details here and there (but don’t, you know, feel obligated to). Regardless, now we have something that’s much easier to digest. One y-axis for general growth rate, and both types of data are represented by the same visual technique. A lot of visual clutter pruned away.&lt;/p&gt;
&lt;p&gt;In the end, it still communicates the core idea: these two values vary together. If anything it’s a little clearer in this case anyway, because we can see that the two lines have the same shape over the years as they rise and fall.&lt;/p&gt;
&lt;p&gt;This turned out to be a surprisingly fun exercise. Looking at a visualization and thinking carefully about how you would do it yourself (better, or just differently) is a great way to practice.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Pre-screen MTurk workers with custom qualifications</title>
      <link>https://katherinemwood.github.io/post/qualifications/</link>
      <pubDate>Mon, 09 Oct 2017 15:25:40 -0500</pubDate>
      
      <guid>https://katherinemwood.github.io/post/qualifications/</guid>
      <description>&lt;p&gt;If you do research using MTurk, you may have run into a common issue: how to recruit exclusively workers that meet certain criteria. For instance, you may want to run a study only on colorblind people, or only people who have perfect pitch, or only people who voted in their last local election. These criteria are all pretty easy to ascertain, but you don’t want to wait to ask until the main HIT itself; then you would be paying full price for data you can’t use at all. However, it’s also not fair to let people accept a HIT, ask them the critical question, and then force them to return it if they don’t meet the necessary criteria. That hurts certain worker metrics like HIT completion rate and can affect their ability to access other HITs, through no real fault of their own.&lt;/p&gt;
&lt;p&gt;One option I’ve seen used is a 2-part HIT. First you make a short, cheap HIT in which you ask the relevant question or two, and then you reach out to the eligible workers who completed that HIT with the link to your actual survey. This is complex and can be time consuming for the requester, though, especially if the pre-screen is extremely short.&lt;/p&gt;
&lt;p&gt;It turns out that there is a very straightforward way to deal with this if you’re willing to use the MTurk web API. You can create custom qualifications that workers can request, either directly or by completing a test, and then assign those qualifications as a requirement for your HITs.&lt;/p&gt;
&lt;p&gt;You can get pretty sophisticated with handling these qualifications, and there is a lot more detail on the &lt;a href=&#34;http://docs.aws.amazon.com/AWSMechTurk/latest/AWSMechanicalTurkRequester/Concepts_QualificationsArticle.html&#34;&gt;AWS documentation pages&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here, I’m going to demonstrate how to make a qualification test that auto-scores itself using the boto3 library for Python. It’s a simple, 2-question colorblindness test (you can view it on the worker sandbox &lt;a href=&#34;https://workersandbox.mturk.com/mturk/requestqualification?qualificationId=3CFGE88WF7UDUETM7YP3RSRD73VS4F&#34;&gt;here&lt;/a&gt;; sign-in required) that first asks people to self-report any known issues with their color vision, and then has them respond to an Ishihara plate. Here’s what it looks like to a worker:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD
&lt;img src=&#34;../img/qualification_test.png&#34; alt=&#34;a qualification test in the wild&#34; /&gt;
=======
&lt;img src=&#34;https://github.com/katherinemwood/katherinemwood.github.io/blob/sources/qualification_test.png&#34; alt=&#34;a qualification test in the wild&#34; /&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt; e08752c6c5d1082b199e2a3b28e25274de7966f2
&lt;p class=&#34;caption&#34;&gt;a qualification test in the wild&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;With this test, instead of asking about colorblindness during the HIT itself and having to throw data out after the fact, I can restrict participation to people who pass this short test.&lt;/p&gt;
&lt;div id=&#34;setup&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Setup&lt;/h2&gt;
&lt;p&gt;To use the MTurk API, a couple of things are neccessary.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://aws.amazon.com/free/?sc_channel=PS&amp;amp;sc_campaign=acquisition_US&amp;amp;sc_publisher=google&amp;amp;sc_medium=cloud_computing_hv_b&amp;amp;sc_content=aws_core_e_control_q32016&amp;amp;sc_detail=aws&amp;amp;sc_category=cloud_computing&amp;amp;sc_segment=188908133959&amp;amp;sc_matchtype=e&amp;amp;sc_country=US&amp;amp;s_kwcid=AL!4422!3!188908133959!e!!g!!aws&amp;amp;ef_id=WUMEpQAAAHy41iUB:20171008205604:s&#34;&gt;an AWS account&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.mturk.com/mturk/welcome&#34;&gt;an MTurk requester account&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://requestersandbox.mturk.com/&#34;&gt;an MTurk sandbox requester account&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://workersandbox.mturk.com/mturk/&#34;&gt;an MTurk sandbox worker account&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;the &lt;a href=&#34;http://boto3.readthedocs.io/en/latest/guide/quickstart.html#installation&#34;&gt;boto3 library for Python&lt;/a&gt;, most easily installed via &lt;a href=&#34;https://pypi.python.org/pypi/pip&#34;&gt;pip&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You’ll also need to link your AWS account to both your real Requester account and to your Sandbox Requester account, and have your access credentials formatted in a way that boto3 can access them.&lt;/p&gt;
&lt;p&gt;For more detail on how to handle setting all of this up, or to see how all the pieces fit together, you can check out &lt;a href=&#34;katherinemwood.github.io/post/mturk_dev_intro/&#34;&gt;this post&lt;/a&gt; for an in-depth guide.&lt;/p&gt;
&lt;p&gt;With that in place, we can move on to actually creating the qualification!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-qualification-skeleton&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. The qualification skeleton&lt;/h2&gt;
&lt;p&gt;Making a qualification itself involves just one function call.&lt;/p&gt;
&lt;p&gt;First, we make our client object:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import boto3

mturk = boto3.client(&amp;#39;mturk&amp;#39;, 
                      region_name=&amp;#39;us-east-1&amp;#39;, 
                      endpoint_url=&amp;#39;https://mturk-requester-sandbox.us-east-1.amazonaws.com&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, to make a qualification, we call &lt;code&gt;create_qualification_type()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;qual_response = mturk.create_qualification_type(
                        Name=&amp;#39;Color blindness test&amp;#39;,
                        Keywords=&amp;#39;test, qualification, sample, colorblindness, boto&amp;#39;,
                        Description=&amp;#39;This is a brief colorblindness test&amp;#39;,
                        QualificationTypeStatus=&amp;#39;Active&amp;#39;,
                        Test=questions,
                        AnswerKey=answers,
                        TestDurationInSeconds=300)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;No matter what kind of qualification you’re creating, you’ll need to give it a &lt;code&gt;Name&lt;/code&gt; and a &lt;code&gt;Description&lt;/code&gt; that workers will see, plus some &lt;code&gt;Keywords&lt;/code&gt; that will help people search for it. We’re going to go ahead and set this qualification to &lt;code&gt;Active&lt;/code&gt; with the &lt;code&gt;QualificationTypeStatus&lt;/code&gt; argument, meaning that it will go live immediately (as live as something can go in the sandbox, that is).&lt;/p&gt;
&lt;p&gt;The next handful of arguments are particular to the qualification type this is–in particular, a qualification type that requires a test. The &lt;code&gt;Test&lt;/code&gt; argument needs to be a &lt;code&gt;QuestionForm&lt;/code&gt; datatype (more on that in a second). This will specify the actual questions that make up the test. If this is left blank, a worker can request the qualification without taking a test first.&lt;/p&gt;
&lt;p&gt;Similarly, the &lt;code&gt;AnswerKey&lt;/code&gt; argument must be an &lt;code&gt;AnswerKey&lt;/code&gt; data structure. This is like an auto-grader for the test. In this file (more details below), we specify how each answer to each question is scored, and what the scoring scheme is overall (a straight sum, a percent, a range, etc). A person will take the test, and if an answer key is provided, be automatically scored. We can then require that people have a certain score on the qualification test to be able to take your HIT. If no answer key is given, you’ll have to go through the tests manually (and free response answers can’t be auto-graded).&lt;/p&gt;
&lt;p&gt;Because we have a qualification test, we have to specify how long people have to take it in seconds with the &lt;code&gt;TestDurationInSeconds&lt;/code&gt; argument. Because this is a short little questionnare, we’ll allot 5 minutes.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-question-file&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. The question file&lt;/h2&gt;
&lt;p&gt;Questions have to be passed in a very specific format. They should be &lt;code&gt;.xml&lt;/code&gt; files, and they have a specific structure. There’s lots of detail &lt;a href=&#34;http://docs.aws.amazon.com/AWSMechTurk/latest/AWSMturkAPI/ApiReference_QuestionFormDataStructureArticle.html&#34;&gt;here&lt;/a&gt;, and you can get pretty sophisticated in terms of your questions.&lt;/p&gt;
&lt;p&gt;In our case, we have just two questions: a text-only question, and a question in which we present an image.&lt;/p&gt;
&lt;p&gt;The XML file needs to start this way:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;&amp;lt;QuestionForm xmlns=&amp;#39;http://mechanicalturk.amazonaws.com/AWSMechanicalTurkDataSchemas/2005-10-01/QuestionForm.xsd&amp;#39;&amp;gt;
...
&amp;lt;/QuestionForm&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This URL is important for technical reasons (it identifies the location of an important file for presenting the questions).&lt;/p&gt;
&lt;p&gt;Inside the &lt;code&gt;QuestionForm&lt;/code&gt; tags are sections for each question. You can give it a name and an identifier (important for matching up the answer key):&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;&amp;lt;Question&amp;gt;
      &amp;lt;QuestionIdentifier&amp;gt;self_report&amp;lt;/QuestionIdentifier&amp;gt;
      &amp;lt;DisplayName&amp;gt;Q1&amp;lt;/DisplayName&amp;gt;
      &amp;lt;IsRequired&amp;gt;true&amp;lt;/IsRequired&amp;gt;
    ...
&amp;lt;/Question&amp;gt;    &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And then we get to the actual question content. This first question is only text:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;&amp;lt;QuestionContent&amp;gt;
        &amp;lt;Text&amp;gt; Which statement best describes your color vision? &amp;lt;/Text&amp;gt;
&amp;lt;/QuestionContent&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But we can also present images, videos, and so on. Here’s how we’ll present the Ishihara plate:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt; &amp;lt;QuestionContent&amp;gt;
        &amp;lt;Text&amp;gt; What number do you see in the image below? &amp;lt;/Text&amp;gt;
        &amp;lt;Binary&amp;gt;
          &amp;lt;MimeType&amp;gt;
            &amp;lt;Type&amp;gt;image&amp;lt;/Type&amp;gt;
            &amp;lt;SubType&amp;gt;jpg&amp;lt;/SubType&amp;gt;
          &amp;lt;/MimeType&amp;gt;
          &amp;lt;DataURL&amp;gt;https://www.spservices.co.uk/images/products/pics/1401209116aw2271.jpg&amp;lt;/DataURL&amp;gt;
          &amp;lt;AltText&amp;gt;Ishihara Color Plate&amp;lt;/AltText&amp;gt;
        &amp;lt;/Binary&amp;gt;
&amp;lt;/QuestionContent&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Specifying the &lt;code&gt;MimeType&lt;/code&gt; within the &lt;code&gt;Binary&lt;/code&gt; tags is what allows us to embed other media.&lt;/p&gt;
&lt;p&gt;After every &lt;code&gt;QuestionContent&lt;/code&gt; section, you have an &lt;code&gt;AnswerSpecification&lt;/code&gt; section. Since we just have radio button responses, we have a &lt;code&gt;SelectionAnswer&lt;/code&gt; section, and then for each answer we have a &lt;code&gt;Selection&lt;/code&gt; with text the worker sees, and then a secret label for that answer that only we see (and that the answer key will use to grade the worker). Like so:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;&amp;lt;AnswerSpecification&amp;gt;
    &amp;lt;SelectionAnswer&amp;gt;
      &amp;lt;StyleSuggestion&amp;gt;radiobutton&amp;lt;/StyleSuggestion&amp;gt;
          &amp;lt;Selections&amp;gt;
            &amp;lt;Selection&amp;gt;
              &amp;lt;SelectionIdentifier&amp;gt;rg&amp;lt;/SelectionIdentifier&amp;gt;
              &amp;lt;Text&amp;gt;I am red-green colorblind.&amp;lt;/Text&amp;gt;
            &amp;lt;/Selection&amp;gt;
            &amp;lt;Selection&amp;gt;
              &amp;lt;SelectionIdentifier&amp;gt;by&amp;lt;/SelectionIdentifier&amp;gt;
              &amp;lt;Text&amp;gt;I am blue-yellow colorblind.&amp;lt;/Text&amp;gt;
            &amp;lt;/Selection&amp;gt;
            &amp;lt;Selection&amp;gt;
              &amp;lt;SelectionIdentifier&amp;gt;other&amp;lt;/SelectionIdentifier&amp;gt;
              &amp;lt;Text&amp;gt;I have some other issue with my color vision.&amp;lt;/Text&amp;gt;
            &amp;lt;/Selection&amp;gt;
            &amp;lt;Selection&amp;gt;
              &amp;lt;SelectionIdentifier&amp;gt;norm&amp;lt;/SelectionIdentifier&amp;gt;
              &amp;lt;Text&amp;gt;My color vision is normal.&amp;lt;/Text&amp;gt;
            &amp;lt;/Selection&amp;gt;
          &amp;lt;/Selections&amp;gt;
    &amp;lt;/SelectionAnswer&amp;gt;
&amp;lt;/AnswerSpecification&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This gets defined for each question.&lt;/p&gt;
&lt;p&gt;Here’s what that entire file, all put together, looks like:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;&amp;lt;QuestionForm xmlns=&amp;#39;http://mechanicalturk.amazonaws.com/AWSMechanicalTurkDataSchemas/2005-10-01/QuestionForm.xsd&amp;#39;&amp;gt;  
  &amp;lt;Question&amp;gt;
      &amp;lt;QuestionIdentifier&amp;gt;self_report&amp;lt;/QuestionIdentifier&amp;gt;
      &amp;lt;DisplayName&amp;gt;Q1&amp;lt;/DisplayName&amp;gt;
      &amp;lt;IsRequired&amp;gt;true&amp;lt;/IsRequired&amp;gt;
      &amp;lt;QuestionContent&amp;gt;
        &amp;lt;Text&amp;gt; Which statement best describes your color vision? &amp;lt;/Text&amp;gt;
      &amp;lt;/QuestionContent&amp;gt;
      &amp;lt;AnswerSpecification&amp;gt;
        &amp;lt;SelectionAnswer&amp;gt;
          &amp;lt;StyleSuggestion&amp;gt;radiobutton&amp;lt;/StyleSuggestion&amp;gt;
          &amp;lt;Selections&amp;gt;
            &amp;lt;Selection&amp;gt;
              &amp;lt;SelectionIdentifier&amp;gt;rg&amp;lt;/SelectionIdentifier&amp;gt;
              &amp;lt;Text&amp;gt;I am red-green colorblind.&amp;lt;/Text&amp;gt;
            &amp;lt;/Selection&amp;gt;
            &amp;lt;Selection&amp;gt;
              &amp;lt;SelectionIdentifier&amp;gt;by&amp;lt;/SelectionIdentifier&amp;gt;
              &amp;lt;Text&amp;gt;I am blue-yellow colorblind.&amp;lt;/Text&amp;gt;
            &amp;lt;/Selection&amp;gt;
            &amp;lt;Selection&amp;gt;
              &amp;lt;SelectionIdentifier&amp;gt;other&amp;lt;/SelectionIdentifier&amp;gt;
              &amp;lt;Text&amp;gt;I have some other issue with my color vision.&amp;lt;/Text&amp;gt;
            &amp;lt;/Selection&amp;gt;
            &amp;lt;Selection&amp;gt;
              &amp;lt;SelectionIdentifier&amp;gt;norm&amp;lt;/SelectionIdentifier&amp;gt;
              &amp;lt;Text&amp;gt;My color vision is normal.&amp;lt;/Text&amp;gt;
            &amp;lt;/Selection&amp;gt;
          &amp;lt;/Selections&amp;gt;
        &amp;lt;/SelectionAnswer&amp;gt;
      &amp;lt;/AnswerSpecification&amp;gt;
  &amp;lt;/Question&amp;gt;
  &amp;lt;Question&amp;gt;
      &amp;lt;QuestionIdentifier&amp;gt;ishihara_39&amp;lt;/QuestionIdentifier&amp;gt;
      &amp;lt;DisplayName&amp;gt;Q2&amp;lt;/DisplayName&amp;gt;
      &amp;lt;IsRequired&amp;gt;true&amp;lt;/IsRequired&amp;gt;
      &amp;lt;QuestionContent&amp;gt;
        &amp;lt;Text&amp;gt; What number do you see in the image below? &amp;lt;/Text&amp;gt;
        &amp;lt;Binary&amp;gt;
          &amp;lt;MimeType&amp;gt;
            &amp;lt;Type&amp;gt;image&amp;lt;/Type&amp;gt;
            &amp;lt;SubType&amp;gt;jpg&amp;lt;/SubType&amp;gt;
          &amp;lt;/MimeType&amp;gt;
          &amp;lt;DataURL&amp;gt;https://www.spservices.co.uk/images/products/pics/1401209116aw2271.jpg&amp;lt;/DataURL&amp;gt;
          &amp;lt;AltText&amp;gt;Ishihara Color Plate&amp;lt;/AltText&amp;gt;
        &amp;lt;/Binary&amp;gt;
      &amp;lt;/QuestionContent&amp;gt;
      &amp;lt;AnswerSpecification&amp;gt;
        &amp;lt;SelectionAnswer&amp;gt;
          &amp;lt;StyleSuggestion&amp;gt;radiobutton&amp;lt;/StyleSuggestion&amp;gt;
          &amp;lt;Selections&amp;gt;
            &amp;lt;Selection&amp;gt;
              &amp;lt;SelectionIdentifier&amp;gt;122&amp;lt;/SelectionIdentifier&amp;gt;
              &amp;lt;Text&amp;gt;122&amp;lt;/Text&amp;gt;
            &amp;lt;/Selection&amp;gt;
            &amp;lt;Selection&amp;gt;
              &amp;lt;SelectionIdentifier&amp;gt;74&amp;lt;/SelectionIdentifier&amp;gt;
              &amp;lt;Text&amp;gt;74&amp;lt;/Text&amp;gt;
            &amp;lt;/Selection&amp;gt;
            &amp;lt;Selection&amp;gt;
              &amp;lt;SelectionIdentifier&amp;gt;21&amp;lt;/SelectionIdentifier&amp;gt;
              &amp;lt;Text&amp;gt;21&amp;lt;/Text&amp;gt;
            &amp;lt;/Selection&amp;gt;
            &amp;lt;Selection&amp;gt;
              &amp;lt;SelectionIdentifier&amp;gt;none&amp;lt;/SelectionIdentifier&amp;gt;
              &amp;lt;Text&amp;gt;I don&amp;#39;t see a number.&amp;lt;/Text&amp;gt;
            &amp;lt;/Selection&amp;gt;
          &amp;lt;/Selections&amp;gt;
        &amp;lt;/SelectionAnswer&amp;gt;
      &amp;lt;/AnswerSpecification&amp;gt;
  &amp;lt;/Question&amp;gt;
&amp;lt;/QuestionForm&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-answer-key&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4. The answer key&lt;/h2&gt;
&lt;p&gt;The answer key looks much the same as the question form. Starts in a similar way:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;&amp;lt;AnswerKey xmlns=&amp;quot;http://mechanicalturk.amazonaws.com/AWSMechanicalTurkDataSchemas/2005-10-01/AnswerKey.xsd&amp;quot;&amp;gt;
  ...
&amp;lt;/AnswerKey&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It also has a &lt;code&gt;Question&lt;/code&gt; section for each question you asked on the test. Within that, you list out the score you want associated with each answer given on the test. For example, for the first question, we want to assign a point only to the answer “my color vision is normal.”&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;&amp;lt;Question&amp;gt;
  &amp;lt;QuestionIdentifier&amp;gt;self_report&amp;lt;/QuestionIdentifier&amp;gt;
  &amp;lt;AnswerOption&amp;gt;
      &amp;lt;SelectionIdentifier&amp;gt;rg&amp;lt;/SelectionIdentifier&amp;gt;
      &amp;lt;AnswerScore&amp;gt;0&amp;lt;/AnswerScore&amp;gt;
    &amp;lt;/AnswerOption&amp;gt;
    &amp;lt;AnswerOption&amp;gt;
      &amp;lt;SelectionIdentifier&amp;gt;by&amp;lt;/SelectionIdentifier&amp;gt;
      &amp;lt;AnswerScore&amp;gt;0&amp;lt;/AnswerScore&amp;gt;
    &amp;lt;/AnswerOption&amp;gt;
    &amp;lt;AnswerOption&amp;gt;
      &amp;lt;SelectionIdentifier&amp;gt;other&amp;lt;/SelectionIdentifier&amp;gt;
      &amp;lt;AnswerScore&amp;gt;0&amp;lt;/AnswerScore&amp;gt;
    &amp;lt;/AnswerOption&amp;gt;
    &amp;lt;AnswerOption&amp;gt;
      &amp;lt;SelectionIdentifier&amp;gt;norm&amp;lt;/SelectionIdentifier&amp;gt;
      &amp;lt;AnswerScore&amp;gt;1&amp;lt;/AnswerScore&amp;gt;
  &amp;lt;/AnswerOption&amp;gt;
&amp;lt;/Question&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Answer values can be anything you like, according to any scoring scheme. They can also be negative!&lt;/p&gt;
&lt;p&gt;After you’re done listing out all of the questions, you need to tell MTurk how to calculate the final score with a &lt;code&gt;QualificationValueMapping&lt;/code&gt; section. There is a lot more detail &lt;a href=&#34;http://docs.aws.amazon.com/AWSMechTurk/latest/AWSMturkAPI/ApiReference_AnswerKeyDataStructureArticle.html&#34;&gt;here&lt;/a&gt;. We’re not going to do anything fancy: just a percentage conversion. We specify a &lt;code&gt;PercentageMapping&lt;/code&gt; and give it the max score to calculate relative to, like so:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;&amp;lt;QualificationValueMapping&amp;gt;
  &amp;lt;PercentageMapping&amp;gt;
    &amp;lt;MaximumSummedScore&amp;gt;2&amp;lt;/MaximumSummedScore&amp;gt;
  &amp;lt;/PercentageMapping&amp;gt;
&amp;lt;/QualificationValueMapping&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And here’s what the entire file looks like, strung together:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;&amp;lt;AnswerKey xmlns=&amp;quot;http://mechanicalturk.amazonaws.com/AWSMechanicalTurkDataSchemas/2005-10-01/AnswerKey.xsd&amp;quot;&amp;gt;
  &amp;lt;Question&amp;gt;
    &amp;lt;QuestionIdentifier&amp;gt;self_report&amp;lt;/QuestionIdentifier&amp;gt;
    &amp;lt;AnswerOption&amp;gt;
      &amp;lt;SelectionIdentifier&amp;gt;rg&amp;lt;/SelectionIdentifier&amp;gt;
      &amp;lt;AnswerScore&amp;gt;0&amp;lt;/AnswerScore&amp;gt;
    &amp;lt;/AnswerOption&amp;gt;
    &amp;lt;AnswerOption&amp;gt;
      &amp;lt;SelectionIdentifier&amp;gt;by&amp;lt;/SelectionIdentifier&amp;gt;
      &amp;lt;AnswerScore&amp;gt;0&amp;lt;/AnswerScore&amp;gt;
    &amp;lt;/AnswerOption&amp;gt;
    &amp;lt;AnswerOption&amp;gt;
      &amp;lt;SelectionIdentifier&amp;gt;other&amp;lt;/SelectionIdentifier&amp;gt;
      &amp;lt;AnswerScore&amp;gt;0&amp;lt;/AnswerScore&amp;gt;
    &amp;lt;/AnswerOption&amp;gt;
    &amp;lt;AnswerOption&amp;gt;
      &amp;lt;SelectionIdentifier&amp;gt;norm&amp;lt;/SelectionIdentifier&amp;gt;
      &amp;lt;AnswerScore&amp;gt;1&amp;lt;/AnswerScore&amp;gt;
    &amp;lt;/AnswerOption&amp;gt;
  &amp;lt;/Question&amp;gt;
  &amp;lt;Question&amp;gt;
    &amp;lt;QuestionIdentifier&amp;gt;ishihara_39&amp;lt;/QuestionIdentifier&amp;gt;
    &amp;lt;AnswerOption&amp;gt;
      &amp;lt;SelectionIdentifier&amp;gt;122&amp;lt;/SelectionIdentifier&amp;gt;
      &amp;lt;AnswerScore&amp;gt;0&amp;lt;/AnswerScore&amp;gt;
    &amp;lt;/AnswerOption&amp;gt;
    &amp;lt;AnswerOption&amp;gt;
      &amp;lt;SelectionIdentifier&amp;gt;21&amp;lt;/SelectionIdentifier&amp;gt;
      &amp;lt;AnswerScore&amp;gt;0&amp;lt;/AnswerScore&amp;gt;
    &amp;lt;/AnswerOption&amp;gt;
    &amp;lt;AnswerOption&amp;gt;
      &amp;lt;SelectionIdentifier&amp;gt;none&amp;lt;/SelectionIdentifier&amp;gt;
      &amp;lt;AnswerScore&amp;gt;0&amp;lt;/AnswerScore&amp;gt;
    &amp;lt;/AnswerOption&amp;gt;
    &amp;lt;AnswerOption&amp;gt;
      &amp;lt;SelectionIdentifier&amp;gt;74&amp;lt;/SelectionIdentifier&amp;gt;
      &amp;lt;AnswerScore&amp;gt;1&amp;lt;/AnswerScore&amp;gt;
    &amp;lt;/AnswerOption&amp;gt;
  &amp;lt;/Question&amp;gt;
  &amp;lt;QualificationValueMapping&amp;gt;
    &amp;lt;PercentageMapping&amp;gt;
      &amp;lt;MaximumSummedScore&amp;gt;2&amp;lt;/MaximumSummedScore&amp;gt;
    &amp;lt;/PercentageMapping&amp;gt;
  &amp;lt;/QualificationValueMapping&amp;gt;
&amp;lt;/AnswerKey&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have these two files, we load them in. We can do this by adding two lines like this at the top of the script that actually makes the qualification:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;questions = open(name=&amp;#39;color_qs.xml&amp;#39;, mode=&amp;#39;r&amp;#39;).read()
answers = open(name=&amp;#39;color_ans_key.xml&amp;#39;, mode=&amp;#39;r&amp;#39;).read()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This reads these files in and assigns their contents to these variables. We then pass these variables into our &lt;code&gt;create_qualification_type&lt;/code&gt; function call as arguments to &lt;code&gt;Test&lt;/code&gt; and &lt;code&gt;AnswerKey&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-qualifications-in-a-hit&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;5. Using qualifications in a HIT&lt;/h2&gt;
&lt;p&gt;So here’s the entire script. Once this script is run, the qualification exists permanently in an active state (unless it’s deleted).&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import boto3

questions = open(name=&amp;#39;color_qs.xml&amp;#39;, mode=&amp;#39;r&amp;#39;).read()
answers = open(name=&amp;#39;color_ans_key.xml&amp;#39;, mode=&amp;#39;r&amp;#39;).read()

mturk = boto3.client(&amp;#39;mturk&amp;#39;, 
                      region_name=&amp;#39;us-east-1&amp;#39;, 
                      endpoint_url=&amp;#39;https://mturk-requester-sandbox.us-east-1.amazonaws.com&amp;#39;)

qual_response = mturk.create_qualification_type(
                        Name=&amp;#39;Color blindness test&amp;#39;,
                        Keywords=&amp;#39;test, qualification, sample, colorblindness, boto&amp;#39;,
                        Description=&amp;#39;This is a brief colorblindness test&amp;#39;,
                        QualificationTypeStatus=&amp;#39;Active&amp;#39;,
                        Test=questions,
                        AnswerKey=answers,
                        TestDurationInSeconds=300)

print(qual_response[&amp;#39;QualificationType&amp;#39;][&amp;#39;QualificationTypeId&amp;#39;])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s important to make a note of the qualification’s unique ID, which is why I print it upon creation (you can also look it up on the MTurk website itself in the URL of the qualification), as this is how we’ll refer to it when we create a HIT that requires this qualification.&lt;/p&gt;
&lt;p&gt;Here’s what that looks like:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;hit = client.create_hit(
        Reward=&amp;#39;0.01&amp;#39;,
        LifetimeInSeconds=3600,
        AssignmentDurationInSeconds=600,
        MaxAssignments=9,
        Title=&amp;#39;A HIT with a qualification test&amp;#39;,
        Description=&amp;#39;A test HIT that requires a certain score from a qualification test to accept.&amp;#39;,
        Keywords=&amp;#39;boto, qualification, test&amp;#39;,
        AutoApprovalDelayInSeconds=0,
        QualificationRequirements=[{&amp;#39;QualificationTypeId&amp;#39;:&amp;#39;3CFGE88WF7UDUETM7YP3RSRD73VS4F&amp;#39;,
                                   &amp;#39;Comparator&amp;#39;: &amp;#39;EqualTo&amp;#39;,
                                   &amp;#39;IntegerValues&amp;#39;:[100]}]
        )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looking at the &lt;code&gt;QualificationRequirements&lt;/code&gt; argument in particular, we can see how this is structured. You pass a list of dictionaries as an argument, with each qualification getting its own dictionary. You can have lots (a location requirement, a minimum HIT approval rating, a minimum number of completed HITs, and so on), but in this case we just have the one. The dictionary has three entries: the qualification type ID, a comparator, and then an integer. These last two tell MTurk how to examine the score to determine if someone is qualified. In our case, we only want to accept workers who had a score exactly equal to 100% (meaning that they did not self-report any vision problems and answered the Ishihara plate correctly), but you could also ask for a score &lt;code&gt;GreaterThan&lt;/code&gt; some minimum value, for example.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;These custom qualifications work really well if they can be automatically scored, and if it’s only a question or two long. It’s not fair to require a long, unpaid test to complete your HIT, unless passing the test would grant them access to multiple, well-paying HITs. For most purposes, a very brief prescreen is all you’ll need.&lt;/p&gt;
&lt;p&gt;This simplifies the process on both sides–workers can quickly get qualified (or not, in which case they can filter those HITs from their options, uncluttering their feeds), and from the requester side, this process is completely automatic. You’ll exclude less data, and won’t have to bother dealing with two separate HITs, invited-only hits, or “bonuses” that are actually payment for the real HIT you want data for.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Getting started with the Mechanical Turk API</title>
      <link>https://katherinemwood.github.io/post/mturk_dev_intro/</link>
      <pubDate>Sun, 08 Oct 2017 15:25:40 -0500</pubDate>
      
      <guid>https://katherinemwood.github.io/post/mturk_dev_intro/</guid>
      <description>&lt;p&gt;Amazon’s Mechanical Turk is a great research tool. You can get huge sample sizes that tend to be more diverse than what you can recruit from a Psychology department subject pool. But, with lots of data comes a new set of problems. It very quickly becomes impossible to manage things manually.&lt;/p&gt;
&lt;p&gt;One very useful tool for dealing with this is the MTurk API. This lets you programmatically access all of MTurk’s functionality, and means you can write scripts to automate big tasks like fetching results, paying workers, or posting large numbers of HITs.&lt;/p&gt;
&lt;p&gt;There is something of a startup cost to this, though. If you’re not sure where to start, or are feeling overwhelmed, here’s a little guide to get you setup with the tools you’ll need.&lt;/p&gt;
&lt;p&gt;This guide is for using the boto3 SDK (Python), but there are also some more general tips about working with AWS.&lt;/p&gt;
&lt;div id=&#34;accounts-users-and-permissions-setup&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Accounts, Users, and Permissions Setup&lt;/h2&gt;
&lt;p&gt;To work programmatically with the MTurk API, you need at minimum two accounts: an AWS account, and an account on the MTurk Requester site. However, you should also make two more accounts: one on the Requester Sandbox, and one on the Worker Sandbox. These last two accounts will let you test and experiment in an environment that looks and behaves just like the real MTurk website, but with none of the consequences.&lt;/p&gt;
&lt;div id=&#34;aws&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;1. AWS&lt;/h3&gt;
&lt;p&gt;AWS stands for “Amazon Web Services.” It’s an umbrella that covers a huge array of the web-based services Amazon offers, including access to their cloud servers. Among these services is MTurk. Your billing info is stored with your AWS account, rather than your MTurk requester account; on the MTurk side of things, you pre-pay for HITs and deplete that purse, rather than having a direct link to your credit card.&lt;/p&gt;
&lt;p&gt;To set up an AWS account, go to the &lt;a href=&#34;https://aws.amazon.com/free/?sc_channel=PS&amp;amp;sc_campaign=acquisition_US&amp;amp;sc_publisher=google&amp;amp;sc_medium=cloud_computing_hv_b&amp;amp;sc_content=aws_core_e_control_q32016&amp;amp;sc_detail=aws&amp;amp;sc_category=cloud_computing&amp;amp;sc_segment=188908133959&amp;amp;sc_matchtype=e&amp;amp;sc_country=US&amp;amp;s_kwcid=AL!4422!3!188908133959!e!!g!!aws&amp;amp;ef_id=WUMEpQAAAHy41iUB:20171008221039:s&#34;&gt;website&lt;/a&gt; and sign up. The free tier of service is all that you’ll need, if you’re only going to be dealing with MTurk.&lt;/p&gt;
&lt;p&gt;One of the many nice features about the AWS account is the ability to create what are called IAM users. These are users that you can assign permissions to without giving them root access to your account. This is the setup I have with my advisor; he has the billing info assigned to his AWS account, but he made me a user with complete MTurk permissions. This means I can post and delete HITs, pay workers, even sign into the AWS console to manage my user settings, but I have no ability to access his AWS account.&lt;/p&gt;
&lt;p&gt;We’ll create an IAM user with credentials that we’ll use to play around in the sandbox, but that don’t grant root access. Once the account is set up and you are signed in to your dashboard, type ‘iam’ in the AWS Services search bar. This will take you to your users dashboard; on the left hand side, go to the &lt;code&gt;users&lt;/code&gt; menu and then click &lt;code&gt;Add User&lt;/code&gt;. You can now set a name for this user; I recommend something obvious, like &lt;code&gt;reqester_sandbox&lt;/code&gt; or &lt;code&gt;mturk&lt;/code&gt;. Below that, you want to check the &lt;code&gt;Enable Programmatic Access&lt;/code&gt; box. On the next page, you can select &lt;code&gt;Attach existing policies directly&lt;/code&gt; (unless you want to deal with setting up groups, which can be useful if you’re managing lab members and multiple people will need the same set of permissions). Since we’re only dealing with MTurk, we don’t have to get too fancy. If you search for &lt;code&gt;mechanical&lt;/code&gt; in the policy search bar, you’ll see one that says &lt;code&gt;AmazonMechanicalTurkFullAccess&lt;/code&gt;. This is the policy we want to attach to this user; it gives full read and write access to MTurk. Check the box next to it and hit &lt;code&gt;Review&lt;/code&gt;, then &lt;code&gt;Create User&lt;/code&gt;. On this next screen, you will be presented with two very important pieces of information; the user access key, and the secret key. This is your only chance to take note of the secret key, so make a note of both of these passwords somewhere. We’ll need them later.&lt;/p&gt;
&lt;p&gt;You’ll also see a special URL that user can use to access the AWS console. This is another piece of information you would want to provide if you were setting up a user for another person.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;amazons-mechanical-turk-requester&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;2. Amazon’s Mechanical Turk – Requester&lt;/h3&gt;
&lt;p&gt;The next step is to create an account on the &lt;a href=&#34;https://requester.mturk.com/&#34;&gt;MTurk requester site&lt;/a&gt;. Once you’ve done that, head over to the &lt;code&gt;Developer&lt;/code&gt; tab and scroll down until you see the &lt;code&gt;Link your AWS Account&lt;/code&gt; option. You’ll need to link these accounts together for programmatic access.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-requester-sandbox&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;3. The Requester Sandbox&lt;/h3&gt;
&lt;p&gt;If you keep scrolling under the &lt;code&gt;Developer&lt;/code&gt; tab on the MTurk requester site, you’ll see a &lt;code&gt;Register for Requester Sandbox&lt;/code&gt; option. Follow this link and make a Sandbox account, and then link it up to your AWS account just like you did on the real MTurk site. The sandbox looks and acts just like the real MTurk, allowing you to do extensive testing on your HITs and qualifications before you launch them for real.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-worker-sandbox&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;4. The Worker Sandbox&lt;/h3&gt;
&lt;p&gt;You can also go over to the &lt;a href=&#34;https://workersandbox.mturk.com/mturk/&#34;&gt;worker sandbox&lt;/a&gt; and make an account there. This will let you see your HITs and qualification tests as real workers will.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;software-setup&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Software Setup&lt;/h2&gt;
&lt;p&gt;In order to handle Mechanical Turk operations via the command line, a few programs have to be installed.&lt;/p&gt;
&lt;div id=&#34;python&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;1. Python&lt;/h3&gt;
&lt;div id=&#34;on-a-mac&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;On a Mac&lt;/h5&gt;
&lt;p&gt;If you are on a Mac, Python is already installed and you don’t need to do anything further.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;on-windows&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;On Windows&lt;/h5&gt;
&lt;p&gt;If you are on windows, you need to install Python 2. You can find Python’s download link &lt;a href=&#34;https://www.python.org/downloads/release/python-2714/&#34;&gt;here&lt;/a&gt;. You’ll want to select &lt;code&gt;Windows x86 MSI Installer&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;pip&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;2. Pip&lt;/h3&gt;
&lt;p&gt;Pip is a package manager for Python that makes it very easy to install additional packages.&lt;/p&gt;
&lt;div id=&#34;on-a-mac-1&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;On a Mac&lt;/h5&gt;
&lt;p&gt;If you are on a Mac, navigate to Pip’s &lt;a href=&#34;https://pip.pypa.io/en/stable/installing/&#34;&gt;website&lt;/a&gt; and download the file &lt;code&gt;get-pip.py&lt;/code&gt;[&lt;a href=&#34;https://bootstrap.pypa.io/get-pip.py&#34; class=&#34;uri&#34;&gt;https://bootstrap.pypa.io/get-pip.py&lt;/a&gt;] (make sure to save it as a .py file if it saves as .txt). Put this file on the desktop.&lt;/p&gt;
&lt;p&gt;Now, open up Terminal, and first enter the following command to navigate to your desktop:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;cd desktop&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we need to run the installation program. Do this by running the following command:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;python get-pip.py&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The program will then run automatically to install pip. You may be prompted for your password at some points during this process.&lt;/p&gt;
&lt;p&gt;To verify that install was successful, run the following command:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;which pip&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You should see a filepath output as a result, telling you where pip was installed. You can now get rid of the installer file if you like.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;on-a-windows&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;On a Windows&lt;/h5&gt;
&lt;p&gt;Since you installed Python, pip comes with it. You may, however, need to upgrade pip. Do so by opening the Command Line and running the following command:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;python -m pip install -U pip&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will upgrade pip to the latest version.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;boto3&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;3. Boto3&lt;/h3&gt;
&lt;p&gt;Now that pip is installed, it’s easy to install the boto3 package. Simply run the following command in either Terminal or Command Line:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;pip install boto3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;IF YOU GET AN ERROR ON MAC:&lt;/strong&gt;&lt;br /&gt;
Try again, but this time run this command:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;sudo pip install boto3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You will be prompted for your password.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;managing-your-aws-credentials&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Managing your AWS credentials&lt;/h2&gt;
&lt;p&gt;As a general rule, you don’t ever want to hardcode your credentials into your code. We want to avoid this at the top of your scripts:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;aws_access_key_id = &amp;#39;my_access_code&amp;#39;
aws_secret_access_key = &amp;#39;my_super_secret_access_key&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The nice thing about boto3 is that it checks a few different places for credentials in a specified order. So, rather than put them in your script directly, you can squirrel them away in a hidden file that the script will access automatically. &lt;a href=&#34;http://boto3.readthedocs.io/en/latest/guide/configuration.html&#34;&gt;This page&lt;/a&gt; has several different options for setting up your credentials; if you’re not comfortable with bash, I suggest making either a shared credentials file or a config file to house your credentials. This is where you’ll put the access key and the secret key that you got from AWS back when you created a new user.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;testing-your-setup&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Testing your setup&lt;/h2&gt;
&lt;p&gt;After all this legwork, it’s time to test the setup! First, you’ll want to copy-paste the code below into a file (adapted from step 5 &lt;a href=&#34;https://requester.mturk.com/developer&#34;&gt;here&lt;/a&gt;) and save it as &lt;code&gt;balance.py&lt;/code&gt;; to make things easy, you might want to save it to the desktop for now.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import boto3

region_name = &amp;#39;us-east-1&amp;#39;

endpoint_url = &amp;#39;https://mturk-requester-sandbox.us-east-1.amazonaws.com&amp;#39;

client = boto3.client(
    &amp;#39;mturk&amp;#39;,
    endpoint_url=endpoint_url,
    region_name=region_name,
)

# This will return $10,000.00 in the MTurk Developer Sandbox
print(client.get_account_balance()[&amp;#39;AvailableBalance&amp;#39;])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can then run it from the Terminal by executing the following commands:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;cd desktop
python balance.py&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If everything is working, you’ll see it print 10,000. You always have 10k in the sandbox. If you call this with the real MTurk site, it will print your actual account balance.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;You’re now setup and ready to start managing MTurk programmatically. With boto3, you can do all sorts of things, including creating and posting hits, contacting workers, fetching results, and managing custom qualifications. This interface allows you to automate some of the more tedious aspects of MTurk, and you can always test everything in the Sandbox.&lt;/p&gt;
&lt;p&gt;Now that you’re good to go, you can check out how to make &lt;a href=&#34;katherinemwood.github.io/post/qualifications/&#34;&gt;custom qualifications for workers using boto3&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Starter tips on sharing data and analysis scripts</title>
      <link>https://katherinemwood.github.io/post/data_sharing_tips/</link>
      <pubDate>Wed, 02 Aug 2017 20:12:31 -0500</pubDate>
      
      <guid>https://katherinemwood.github.io/post/data_sharing_tips/</guid>
      <description>&lt;p&gt;One practice that’s considered increasingly important to transparency and reproducibility is open data (and the accompanying analysis script). Of course, it’s one thing to say “just post it!” It can be overwhelming if you’re new to the practice. There are different considerations when posting data publicly than when you’re retaining it solely for internal use. I’ve outlined a few things to consider when posting your data, and a few tips to help make it more accessible and usable for others who want to access it.&lt;/p&gt;
&lt;div id=&#34;subject-consent-and-irb-approval&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;1. Subject consent and IRB approval&lt;/h1&gt;
&lt;p&gt;If you intend to publicly post your data, a good place to start is updating your consent forms through the IRB to have some text about it, so that subjects can give their informed consent to have their data shared. For example, this is what the relevant text in our lab looks like for our subject pool recruits:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Your responses will be assigned a code number that is not linked to your name or other identifying information. All data and consent forms will be stored in a locked room or a password-protected digital archive. Results of this study may be presented at conferences and/or published in books, journals, and/or the popular media. &lt;strong&gt;De-identified data may be made publicly avaliable.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That’s it! Then you can be assured in your studious data posting.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;anonymization&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;2. Anonymization&lt;/h1&gt;
&lt;p&gt;If you have data that uses some sort of non-random identifier for each subject, you’ll want to strip that out and replace it with some arbitrary subject ID before you post it. MTurk Worker IDs, for example, are not strictly anonymous, because they are linked to a person’s real information. You might want them during data collection, but not for public data.&lt;/p&gt;
&lt;p&gt;Even if you don’t collect subjects’ names or other explicitly identifying information, it can still be possible to identify someone from the information in their data. A particular combination of gender, general age range, workplace, and ethnicity in a survey of professors, for example, might identify someone. You don’t have to be hyper-vigilant about everything all of the time, but it’s not a bad idea to keep this in mind and possibly take steps around it, such as withholding or binning nonessential variables, if it seems like it would be an issue.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;file-format&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;3. File format&lt;/h1&gt;
&lt;p&gt;For maximum accessibility, plaintext is your friend. If at all possible, text should be in .txt format, column-data should be in .csv format, and images should be in .jpg, .png, .tiff, or some other common form. If possible, you want to avoid having data in proprietary forms, like .psd (Photoshop) which some people may not be able to open. Word and Excel documents can behave in strange ways if you open them in other applications, and sometimes aren’t back-compatible with older version of the software. You want anyone to be able to open your files, regardless of their setup.&lt;/p&gt;
&lt;p&gt;Similarly, you want your analysis code in its source format and in plaintext. You don’t want to wrap things in executables, or post code as an image or word-processed document.&lt;/p&gt;
&lt;p&gt;Data should also be in their raw form. Cleaning, wrangling, and summarizing should be left to the scripts if possible.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;readmes&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;4. Readmes&lt;/h1&gt;
&lt;p&gt;You should include, along with your data and/or script, a readme that tells the user what they need to know. How do they run the analysis? Are there any special steps they need to take? What should things look like as it happens? What kind of output should they expect?&lt;/p&gt;
&lt;p&gt;You also need a readme for your data. What variables are included in the dataset? What is the datatype of each variable? If you opened up a new datafile and see a column of numbers ranging from 1 to 4, that could mean anything. Did it come from a Likert-type scale? Is it a count of some event? Is it a continuous variable, or a discrete one? This confusion is only worsened by unusual or non-descriptive variable name choices, like &lt;code&gt;psc_sf&lt;/code&gt;. A detailed readme clears this up not only for the user, and but possibly for your future self!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;future-proofing&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;5. Future-proofing&lt;/h1&gt;
&lt;p&gt;If your analysis has a lot of dependencies–it loads a lot of packages, or requires something special–it’s very possible that it could break in the future with updates and new versions, or just not work for a user if they have a different setup. If you don’t want to be constantly maintaining your script, or trying to anticipate what things will look like for someone else, there are tools you can use that will wrap everything up for you in a bundle so that your project will always have the set of dependencies it needs. Here are a few:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.docker.com/&#34;&gt;Docker&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;https://www.reprozip.org/&#34;&gt;ReproZip&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;https://github.com/benmarwick/rrtools&#34;&gt;&lt;code&gt;rrtools&lt;/code&gt; package&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;https://rstudio.github.io/packrat/&#34;&gt;PackRat&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you have a lightweight setup, you may not need these measures. But if it’s a concern, you have options!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;forever-homes&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;6. Forever homes&lt;/h1&gt;
&lt;p&gt;Ideally, you’ll want to host your resources in a reliable place that won’t give you or your users access problems, is content to host multiple filetypes, and makes retrieval easy. The &lt;a href=&#34;https://osf.io&#34;&gt;Open Science Framework&lt;/a&gt; is a great option, as is &lt;a href=&#34;https://github.com/&#34;&gt;GitHub&lt;/a&gt;. Plus, this can serve as your cloud-backup in case some cataclysm destroys your computer.&lt;/p&gt;
&lt;p&gt;This also makes it possible to have the analysis script download the data directly for a user, rather than relying on them to put it in the right place. For instance, you may have a line in your private version of the script that sets the current directory to &lt;code&gt;users/me/documents/folder_with_the_stuff&lt;/code&gt;, which the user would have to change if you uploaded it as-is.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Posting all this publicly can feel intimidating, especially if it’s a new experience. But the most important thing is that it’s out there. It doesn’t have to be perfect, and it gets easier and better with practice.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Some thoughts on the New Significance</title>
      <link>https://katherinemwood.github.io/post/new_sig/</link>
      <pubDate>Wed, 26 Jul 2017 10:00:17 -0700</pubDate>
      
      <guid>https://katherinemwood.github.io/post/new_sig/</guid>
      <description>&lt;p&gt;The busy corridors of psychology twitter have been abuzz over a &lt;a href=&#34;https://osf.io/preprints/psyarxiv/mky9j/&#34;&gt;new paper&lt;/a&gt;, written by some heavy-hitters in the field, arguing for a new threshold for statistical significance. Now, instead of &lt;span class=&#34;math inline&#34;&gt;\(p &amp;lt; .05\)&lt;/span&gt;, the threshold would be &lt;span class=&#34;math inline&#34;&gt;\(p &amp;lt; .005\)&lt;/span&gt;. If you’ve not read the paper, I recommend doing so; it’s short and won’t take too much time.&lt;/p&gt;
&lt;p&gt;The argument isn’t new–there was a 2013 paper arguing for something similar by Johnson–but it has made quite a splash this time.&lt;/p&gt;
&lt;p&gt;As a way of clarifying my own thoughts on this, I thought I’d work through the main discussion points around this proposal that I’ve encountered on Twitter. This isn’t comprehensive by a long shot. People had a &lt;em&gt;lot&lt;/em&gt; to say about this. But these were the most common arguments.&lt;/p&gt;
&lt;div id=&#34;this-wont-fix-nhst&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;1. This won’t fix NHST&lt;/h1&gt;
&lt;p&gt;This is probably the most common point. It’s true, of course–all the same issues with NHST (null-hypothesis significance testing), even when it is properly used, don’t disappear at &lt;span class=&#34;math inline&#34;&gt;\(p &amp;lt; .005\)&lt;/span&gt;. People opposed to NHST in general won’t ever be pleased by changes like this; generally they want to see a move to estimation (effect sizes and confidence/credible intervals) and/or Bayesian methods. So do many of the authors on this paper, in fact.&lt;/p&gt;
&lt;p&gt;I’m a fan of the effect size/estimation approach to reporting, and I’d like to see it used more widely (or at least always alongisde p-values, if we’re starting gradually). On the other hand, change is slow; a lot of the issues we’re discussing now have been discussed decades before, to little effect. In the meantime, it might not be an outrageous idea to be more stringent. &lt;span class=&#34;math inline&#34;&gt;\(p &amp;lt; .005\)&lt;/span&gt; is &lt;em&gt;much&lt;/em&gt; harder to p-hack than &lt;span class=&#34;math inline&#34;&gt;\(p &amp;lt; .05\)&lt;/span&gt;, and as the authors point out in the paper, it drops the “false discovery rate” across the board for all levels of power.&lt;/p&gt;
&lt;p&gt;If you don’t like p-values and would like to see them discontinued, then proposals like this are going to look like so much wasted effort. But, there is something to be said for the fact that the system can’t be changed overnight and seeing what changes we can make in the meantime.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;this-encourages-dichotomous-thinking&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;2. This encourages dichotomous thinking&lt;/h1&gt;
&lt;p&gt;Related to #1, one argument against that I’ve seen articulated by a few different people is that this proposal doesn’t fix the issue of the “significant-nonsignificant” dichotomy. This point is hard to argue with. The authors in the paper suggest treating &lt;span class=&#34;math inline&#34;&gt;\(p &amp;lt; .05\)&lt;/span&gt; as “suggestive,” not unlike how there is the tendency for people to call &lt;span class=&#34;math inline&#34;&gt;\(p &amp;lt; .1\)&lt;/span&gt; “trending towards significance.” But, as some of these folks have pointed out, that’s still entirely based on these thresholds. It’s essentially letting the p-value do the thinking for you, rather than demanding more difficult, nuanced interpretation.&lt;/p&gt;
&lt;p&gt;I think there’s definitely something to this point. When these thresholds become really important– either because they are defacto required for publication, or required to avoid having others dismiss your work–results can get distilled down to the p-value and nothing more. For instance, an effect that is small, highly variable, and difficult to replicate even within the same lab is held up as “real” because it meets the threshold; the sixth of seven unplanned tests limbos under the cutoff, and it’s latched onto as the “important” result, and so on. The p-value becomes more important than whether or not the effect was predicted, how big it is, or how reliable it is.&lt;/p&gt;
&lt;p&gt;I think this concern is reasonable. But, related to #1, change is gradual, and p-values are going to be here for a while yet (and may be here forever). So, is there any harm in a lower significance threshold? Sure, it might not &lt;em&gt;help&lt;/em&gt; in terms of the philosophy of the p-value and NHST, but if these are your primary concerns, it can’t hurt, either. Unless you view it as a waste of time and resources (see #3).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;change-the-culture-not-the-arbitrary-cutoffs&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;3. Change the culture, not the arbitrary cutoffs&lt;/h1&gt;
&lt;p&gt;Why not both? This is somewhat related to #1 and #2. The argument goes that we should be spending time and effort pushing for systematic cultural change. We definitely should! Things would be better if “open science practices” were just “science practices.” But I don’t think this is zero-sum. I think we can keep trying to change the culture &lt;em&gt;and&lt;/em&gt; change the arbitrary cutoffs at the same time, particularly if you think that changing the cutoffs could be a force for good.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;this-will-throw-the-type-i-to-type-ii-error-ratio-out-of-whack&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;4. This will throw the Type I to Type II error ratio out of whack&lt;/h1&gt;
&lt;p&gt;A Type I error is when you reject the null hypothesis when in reality it should have been retained; a Type II error is when you retain the null when you should have rejected it. Typically, part of NHST is striking a balance between these two errors. The alpha level (significance threshold) is the rate of Type I errors we are willing to accept. To put things very simply, the Type II error rate is controlled by power; how often will you detect a given effect size with your chosen sample size?&lt;/p&gt;
&lt;p&gt;At the traditional values of &lt;span class=&#34;math inline&#34;&gt;\(\alpha = .05\)&lt;/span&gt; and 80% power, the ratio of Type I to Type II errors is 1:4. Under the new threshold, this ratio would be 1:40, which for some people is far too skewed.&lt;/p&gt;
&lt;p&gt;The authors actually address this point in the paper, and &lt;a href=&#34;http://sometimesimwrong.typepad.com/wrong/2017/07/alpha-wars.html&#34;&gt;Simine Vazire covers it on her blog&lt;/a&gt;. The short answer is that, right now, with p-hacking, multiple testing, and a lack of preregistration, our alpha level isn’t actually what it seems. Power, too, is often much lower than the typically-recommended 80% level, and &lt;a href=&#34;https://www.refsmmat.com/notebooks/power.html&#34;&gt;hasn’t improved much&lt;/a&gt; since Coehn first wrote about it back in 1962. In practice, this means that the error ratio isn’t 1:4. As Simine points out, the 1:40 ratio probably would be too strict in the case of systematic preregistration and other changes. We’ve got a ways to go before we need to worry about that, though, since we’ve only just started to publicize and prevent the effects of HARKing, QRPs, and so on.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;this-will-make-publication-harder-what-about-expensive-data-and-small-populations-of-interest&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;5. This will make publication harder / What about expensive data and small populations of interest&lt;/h1&gt;
&lt;p&gt;The authors stress that this threshold should not be used as a publication criterion. This is important, because using these cutoffs as requirements for publication inflate the effect size estimates in the literature. You only avoid biased estimates if there is no publication bias. This doesn’t mean it wouldn’t be adopted as a requirement, of course, but the authors are advocating this as a standard-of-evidence metric, not a publication requirement. (Insert the line about the paving material on the road to hell here.)&lt;/p&gt;
&lt;p&gt;As for the second points, this was also true at &lt;span class=&#34;math inline&#34;&gt;\(p &amp;lt; .05\)&lt;/span&gt;. I’ve also seen this point brought up in response to other proposed changes to the field, like encouraging more power. I don’t have much perspective to offer here, because I work with undergrads and MTurkers as subjects and have little difficulty getting enough people. There’s &lt;a href=&#34;https://twitter.com/chrisdc77/status/890120245423345664&#34;&gt;a little thread here&lt;/a&gt; about this sort of thing by people smarter than I am.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-does-this-actually-look-like&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What does this actually look like?&lt;/h1&gt;
&lt;p&gt;I’ve done some simulations looking at p-hacking, etc. with the &lt;span class=&#34;math inline&#34;&gt;\(p &amp;lt; .05\)&lt;/span&gt; significance level. It’s helped me develop some intuitions, but I’m curious about the new significance level. I’d like to get to know it a little better.&lt;/p&gt;
&lt;p&gt;So, I’m going to do some simulating. First, I want to look at how many significant results you’ll get at various effect sizes at various sample sizes. I’m going to simulate the plain old t-test because, hey, it’s just a first date.&lt;/p&gt;
&lt;p&gt;I want to see how “hits” (getting a value below the significance threshold when there is a real effect, aka power) varies with sample size and effect size between the two cutoffs.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;suppressPackageStartupMessages({
  library(data.table)
  library(dplyr)
  library(ggplot2)
  library(tidyr)
  library(viridis)
})

 sim.es &amp;lt;- function(n, es) {
    g1 &amp;lt;- rnorm(n, es, 1)
    g2 &amp;lt;- rnorm(n, 0, 1)
    c(&amp;#39;p&amp;#39;=t.test(g1, g2, paired=FALSE)$p.value, &amp;#39;n&amp;#39;=n, &amp;#39;es&amp;#39;=es)
 }
params &amp;lt;- expand.grid(list(&amp;#39;n&amp;#39;=c(20, 50, 100, 500), &amp;#39;es&amp;#39;=seq(0, 1, .1)))
simulation_results &amp;lt;- rbindlist(mapply(function(n, es) 
  as.data.frame(t(replicate(1000, sim.es(n, es)))), 
                             n=params$n, es=params$es, SIMPLIFY=FALSE))

sim_graph &amp;lt;- ggplot(data=simulation_results %&amp;gt;% 
                      group_by(n, es) %&amp;gt;% 
                      summarize(sig_results_new=sum(p &amp;lt; .005)/1000,
                                sig_results_old=sum(p &amp;lt; .05)/1000) %&amp;gt;%
                      gather(group, sig_pct, sig_results_old, sig_results_new)) +
  geom_line(aes(x=es, y=sig_pct, lty=group, color=as.factor(n))) +
  scale_color_viridis(discrete=TRUE) +
  scale_linetype_discrete(labels=c(&amp;#39;p &amp;lt; .005&amp;#39;, &amp;#39;p &amp;lt; .05&amp;#39;)) +
  labs(color=&amp;#39;N per cell&amp;#39;, lty=&amp;#39;Alpha level&amp;#39;) +
  xlab(&amp;#39;Effect Size&amp;#39;) +
  ylab(&amp;#39;Percent significant results&amp;#39;) +
  theme_minimal()
sim_graph&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../&#34;katherinemwood.github.io/&#34;  # End your URL with a `/` trailing slash.post/new_sig_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So, yeah. Even at a big effect size, you’re not getting away with 20 per cell under the .005 threshold. Even 50 per cell is pretty lean unless you’re hunting a big effect.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;so&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;So…&lt;/h1&gt;
&lt;p&gt;If nothing else, you’ve got to hand it to this paper for sparking a diverse and extensive discussion about where the field is now, and how best to encourage the changes that will make our science more robust.&lt;/p&gt;
&lt;p&gt;I know you’re waiting with bated breath to hear my personal opinion on this. Call me an ideological slouch, but after reading the paper, and reading the arguments for and against, I remain solidly middle-ground. I think it’s true that changing this threshold wouldn’t change much; it doesn’t fix NHST or dichotomous, significance-based thinking. Many of the old problems would persist under the new world order. On the other hand, I don’t think this proposal is actively harmful (although others would vehemently disagree with me on that point), and I think it would even help a bit in terms of going to the mat for more power and higher standards of evidence.&lt;/p&gt;
&lt;p&gt;This is definitely not the Thing That Will Save Psych Forever, and the authors don’t claim that it is. But I think it’s an interesting idea that has potential benefits, and merits consideration. It should also make us think about the strength of evidence we want to see, and how we should go about trying to effect that change in the field. Requiring a stricter cutoff is certainly a place to start.&lt;/p&gt;
&lt;p&gt;If you’re interested in following this discussion, there’s a lot of it on Twitter. People are also getting into the mathematics behind the Bayesian perspective on p-values, prior probabilities, and so on; it’s worth a look if you’re interested. If this paper accomplishes nothing else, it’s sparked some interesting discussions.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Oh, joy! Joyplots in R with ggjoy</title>
      <link>https://katherinemwood.github.io/post/joy/</link>
      <pubDate>Sat, 15 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://katherinemwood.github.io/post/joy/</guid>
      <description>&lt;p&gt;I’ve &lt;a href=&#34;https://katherinemwood.github.io/post/violins/&#34;&gt;written before&lt;/a&gt; about plots that are more informative than your standard barplot.&lt;/p&gt;
&lt;p&gt;Another option is the joyplot (also known as &lt;a href=&#34;http://www.brendangregg.com/frequencytrails.html&#34;&gt;frequency trails&lt;/a&gt;). Joyplots are like mountain ranges, except instead of mountains it’s smoothed density histograms. It’s a way to visualize a lot of data in a pretty compact space.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/clauswilke/ggjoy&#34;&gt;Claus Wilke has written an R package&lt;/a&gt; that makes plotting these pretty frictionless. Just add data!&lt;/p&gt;
&lt;p&gt;I’ll whip up some fake data to play with.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
joy &amp;lt;- data.frame(&amp;#39;label&amp;#39;=rep(letters[1:10], each=100),
                  &amp;#39;value&amp;#39;=as.vector(mapply(rnorm, rep(100, 10), rnorm(10), SIMPLIFY=TRUE)),
                  &amp;#39;rank&amp;#39;=rep(1:5, each=100, times=20))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s an example in which the distributions are allowed to overlap vertically, giving the appearance that they are stacked.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;suppressPackageStartupMessages({
  library(ggjoy)
  library(dplyr)})

ggplot(joy, aes(x=value, y=label)) +
  geom_joy(scale=2, rel_min_height=.03) +
  scale_y_discrete(expand = c(0.01, 0)) +
  xlab(&amp;#39;Value&amp;#39;) +
  theme_joy() +
  theme(axis.title.y = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Picking joint bandwidth of 0.209&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../&#34;katherinemwood.github.io/&#34;  # End your URL with a `/` trailing slash.post/joy_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt; To make things look more orderly, and since our labels have no inherent order, we can order the data according to the mean. This isn’t neccessary if your y-axis is time, for instance.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;joy &amp;lt;- group_by(joy, label) %&amp;gt;%
       mutate(m=mean(value)) %&amp;gt;%
       arrange(m) %&amp;gt;%
       ungroup() %&amp;gt;%
       mutate(label=factor(label, unique(label)))

ggplot(joy, aes(x=value, y=label)) +
  geom_joy(scale=2, rel_min_height=.03) +
  scale_y_discrete(expand = c(0.01, 0)) +
  xlab(&amp;#39;Value&amp;#39;) +
  theme_joy() +
  theme(axis.title.y = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Picking joint bandwidth of 0.209&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../&#34;katherinemwood.github.io/&#34;  # End your URL with a `/` trailing slash.post/joy_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Some people don’t like this overlap (for the aesthetics or because it can obscure the data) so here’s an example if we spread everybody out so that they don’t touch. The amount of overlap is easily controlled by the &lt;code&gt;scale&lt;/code&gt; argument in geom_joy. &lt;code&gt;scale = 1&lt;/code&gt; means no overlap; the higher this value, the more the distributions will overlap.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(joy, aes(x=value, y=label)) +
  geom_joy(scale=1, rel_min_height=.01) +
  scale_y_discrete(expand = c(0.001, 0)) +
  xlab(&amp;#39;Value&amp;#39;) +
  theme_joy() +
  theme(axis.title.y = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Picking joint bandwidth of 0.209&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../&#34;katherinemwood.github.io/&#34;  # End your URL with a `/` trailing slash.post/joy_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;How about some color? Color can be used to separate groups:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(joy, aes(x=value, y=label, fill=as.factor(rank))) +
  geom_joy(scale=1, rel_min_height=.01) +
  scale_y_discrete(expand = c(0.01, 0)) +
  xlab(&amp;#39;Value&amp;#39;) +
  labs(fill=&amp;#39;Rank&amp;#39;) +
  theme_joy() +
  theme(axis.title.y = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Picking joint bandwidth of 0.209&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../&#34;katherinemwood.github.io/&#34;  # End your URL with a `/` trailing slash.post/joy_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Or, if your data doesn’t have grouping structure, it can just be used to break things up a little bit. (See &lt;a href=&#34;https://twitter.com/hnrklndbrg/status/883675698300420098&#34;&gt;this tweet&lt;/a&gt; for another example.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(joy, aes(x=value, y=label, fill=label)) +
  geom_joy(scale=3, rel_min_height=0.01) +
  scale_fill_manual(values=rep(c(&amp;#39;gray&amp;#39;, &amp;#39;lightblue&amp;#39;), length(unique(joy$label))/2)) + 
  scale_y_discrete(expand = c(0.01, 0)) +
  xlab(&amp;#39;Value&amp;#39;) +
  theme_joy() +
  theme(axis.title.y = element_blank(),
        legend.position=&amp;#39;none&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Picking joint bandwidth of 0.209&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../&#34;katherinemwood.github.io/&#34;  # End your URL with a `/` trailing slash.post/joy_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;How about some joyful facets?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(filter(joy, rank != 5), aes(x=value, y=label)) +
  geom_joy(scale=1, rel_min_height=.03) +
  scale_y_discrete(expand = c(0.01, 0)) +
  xlab(&amp;#39;Value&amp;#39;) +
  facet_wrap(~rank, scales=&amp;#39;free&amp;#39;) +
  theme_joy() +
  theme(axis.title.y = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Picking joint bandwidth of 0.212&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../&#34;katherinemwood.github.io/&#34;  # End your URL with a `/` trailing slash.post/joy_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You can check out &lt;a href=&#34;https://cran.r-project.org/web/packages/ggjoy/vignettes/introduction.html&#34;&gt;the vignette&lt;/a&gt; for more examples and more information. You can also &lt;a href=&#34;https://cran.r-project.org/web/packages/ggjoy/ggjoy.pdf&#34;&gt;check out the docs&lt;/a&gt; for information on, for example, how the density is calculated and what your options are for that.&lt;/p&gt;
&lt;p&gt;If you want to play around with these yourself, you can make up data (like I did) or check out the examples in the vignette for datasets to use that will make cool plots. Who knows–you might even find the iris dataset fun again if you use it to make joyplots.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Considerations when writing a preregistration if you&#39;re new to all this</title>
      <link>https://katherinemwood.github.io/post/prereg/</link>
      <pubDate>Mon, 26 Jun 2017 16:11:46 -0500</pubDate>
      
      <guid>https://katherinemwood.github.io/post/prereg/</guid>
      <description>&lt;p&gt;(Note: I apparently use preregistration and pre-registration interchangeably, because I guess I want to have my hyphenation cake and eat it, too.)&lt;/p&gt;
&lt;p&gt;I’m a selfish preregisterer (Preregisterizer? Preregistrar? Preregistrix?). I preregister experiments primarily for my own benefit. I make as many decisions as I can up front, when I’m unbiased and unattached. Yes, it’s more transparent, and yes, it provides a record to others that you made your decisions ahead of time and not in a post-hoc way. But I mostly do it for myself.&lt;/p&gt;
&lt;p&gt;Like Odysseus tying himself to the mast, preregistration saves you from the siren song of totally justifiable post-hoc decisions that just happen to make things work a little more nicely. I do think that usually this isn’t ill-intentioned or even necessarily deliberate, but I do think it is extremely hard (if not impossible) to make decisions in an unbiased way once the data have started coming in.&lt;/p&gt;
&lt;p&gt;Preregistration can be an asset, not an extra chore. It eliminates decision points, which speeds up the process considerably (especially in the analysis stage). It provides a record to yourself of your methods, analysis intentions, and motivating rationale. It also, of course, provides this same record to others.&lt;/p&gt;
&lt;p&gt;So, what are some things you might want to think about when writing a pre-registration if you’re just starting out?&lt;/p&gt;
&lt;div id=&#34;what-are-your-hypotheses-and-predictions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;1. What are your hypotheses and predictions?&lt;/h1&gt;
&lt;p&gt;Why are you running this study? What hypotheses are you interested in testing, or what theories are you exploring? Do you have predictions about how the data will look, or a set of possible outcomes depending on what mechanisms are at work? Can you generate hypothetical/example data for these possibilities?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-many-subjects&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;2. How many subjects?&lt;/h1&gt;
&lt;p&gt;How many people are you going to run, and why? Maybe you do a power analysis to figure out how many you would need to find your smallest effect size of interest, maybe you just always like to run 50 people per group. However you decide on your number, you want to decide on this upfront.&lt;/p&gt;
&lt;p&gt;Alternatively, if you’re going to be doing a sequential analysis, where you periodically inspect the data over the course of collection and stop once you’ve reached some threshold (such as a Bayes factor of 10 in either support of the alternative or support of the null), you’ll want to specify those details. What is the stopping point? How many batches will you run?&lt;/p&gt;
&lt;p&gt;How will you be recruiting people? What’s their compensation? Are their any inclusion criteria they have to meet (right-handed, normal or corrected-normal vision, etc)?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-are-your-methods&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;3. What are your methods?&lt;/h1&gt;
&lt;p&gt;How are you going to run the study? To address this, I usually like to just write the methods section. I think it’s important to be detailed here because you want to prevent nudging of the experimental procedure if it “doesn’t seem to be working.” If possible, can you upload your experimental script as it will be run and all of the stimuli?&lt;/p&gt;
&lt;p&gt;If you always test in the same place in the same way, you could write up a general equipment page that you link to in addition to the details of your particular study (if you always use the same eyetracker, for example).&lt;/p&gt;
&lt;p&gt;This will make writing the methods section a breeze when the time comes!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-is-your-analysis-plan&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;4. What is your analysis plan?&lt;/h1&gt;
&lt;p&gt;How are you going to analyze the data? What kind of tests are you going to do, and between which conditions? Are there conditions under which you will pool observations across conditions? You want to make all of these decisions before you have any idea what the data are. .&lt;/p&gt;
&lt;p&gt;If possible, can you upload the very script you’ll use to analyze the data?&lt;/p&gt;
&lt;p&gt;How are you going to code free-responses? If people have to be classified, how are you going to do it? In the research I do, I run a lot of inattentional blindness studies. One of the things we have to decide is how to determine “noticers” (people who saw the unexpected event) from “nonnoticers” (people who missed it). This is a very consequential decision, and is definitely not one we want to have any flexibility over after the data have come in.&lt;/p&gt;
&lt;p&gt;Are you going to exclude subjects or observations? Under what conditions? Try to imagine every possible reason you might need to exclude someone and list it. These can be things like failing attention checks, performing below chance on some critical accuracy measure, falling asleep during the experiment, reporting colorblindness during a color-based study, and so on.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;practice-makes-perfect&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Practice makes perfect&lt;/h1&gt;
&lt;p&gt;I wasn’t sure about this whole preregistration thing when I first started. It seemed like just another chore, and I was on the up and up, so why did I need to do this, anyway? But after I ran my first preregistered experiment, I came to appreciate the absence of flexibility. Even in what I expected to be a completely straightforward experiment, there were dozens of little decisions I could have made in the moment. It was so freeing to be able to just stick to the plan and have all the decisions made ahead of time. No agonizing, no doubting, no trying a million different things in the off-chance that some other analysis would look better. I just followed my map right out of the garden of forking paths.&lt;/p&gt;
&lt;p&gt;If you want to start pre-registering your work, the &lt;a href=&#34;https://osf.io/&#34;&gt;Open Science Framework&lt;/a&gt; is a great option. There’s also AsPredicted.&lt;/p&gt;
&lt;p&gt;Preregistering a study for the first time can feel strange, particularly if you’re accustomed to running non-preregistered studies. When you’re used to dealing with problems and decision points as they emerge over the course of a study, rather than anticipating them ahead of time, it can be hard to even enumerate them. Like anything, though, it gets much easier with practice.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>One day build: Shiny effect sizes</title>
      <link>https://katherinemwood.github.io/post/shiny_es/</link>
      <pubDate>Mon, 26 Jun 2017 16:09:00 -0500</pubDate>
      
      <guid>https://katherinemwood.github.io/post/shiny_es/</guid>
      <description>&lt;p&gt;It had been a long time since I had built anything, and I needed to scratch the itch. I decided to build a Shiny port of &lt;a href=&#34;https://osf.io/ixGcd/&#34;&gt;Daniël Lakens’s beloved effect size calculator spreadsheets&lt;/a&gt;. You can find the app itself &lt;a href=&#34;https://katherinemwood.shinyapps.io/lakens_effect_sizes/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;These sheets are handy for a number of reasons, primarily if a paper or talk reports stats but no effect sizes (or if you want to double-check reported stats or effect sizes). I thought it would be convenient to have an online version.&lt;/p&gt;
&lt;p&gt;All of the math is based on what’s used in the spreadsheet, and the default examples are taken from there too.&lt;/p&gt;
&lt;p&gt;This was a really fun build, mostly because the actual math was pretty straightforward (and I could easily check it against the spreadsheets to make sure it was working, which is a refreshing change). I got to spend more time digging into the web-dev side of things, dealing with layout and the reactivity of the components of the app. I learned a lot, and it was great to be working with Shiny again.&lt;/p&gt;
&lt;p&gt;The code is &lt;a href=&#34;https://github.com/katherinemwood/lakens_effect_sizes&#34;&gt;on Github&lt;/a&gt;. Suggestions for improvements and bug reporting encouraged!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Intro to unit testing in R</title>
      <link>https://katherinemwood.github.io/post/testthat/</link>
      <pubDate>Mon, 26 Jun 2017 15:46:23 -0500</pubDate>
      
      <guid>https://katherinemwood.github.io/post/testthat/</guid>
      <description>&lt;p&gt;I’ve mentioned before that a great coding practice you can ingrain into your work is unit testing. The idea is simple: you write a script that automatically evaluates pieces of your code and checks it against expected behavior. For example, a correlation coefficient should always be between -1 and 1, so you could write a test that will raise an error if it encounters an r beyond these values. Or, you could check your data file after it’s read in to make sure it reads in with all rows and columns, nothing missing where it shouldn’t be, and every column of the data type you want it.&lt;/p&gt;
&lt;p&gt;Hadley Wickham wrote an awesome R package that makes writing tests easy and pretty intuitive. In the &lt;code&gt;testthat&lt;/code&gt; package, you can bundle a series of &lt;code&gt;expect_that&lt;/code&gt; statements into a &lt;code&gt;test_that&lt;/code&gt; suite, which ideally would run a small cluster of closely related tests. Multiple &lt;code&gt;test_that&lt;/code&gt; suites can be grouped into a &lt;code&gt;context&lt;/code&gt; that tests an entire file, or a big chunk of functionality inside a file.&lt;/p&gt;
&lt;p&gt;I’ll walk through examples. Code for this is up on GitHub, like always.&lt;/p&gt;
&lt;p&gt;So first, to get things set up, you have a file that you want to test, and then you make a test script to go along with it. If you want to set up automated testing, name this file &lt;code&gt;test_something.R&lt;/code&gt;. &lt;code&gt;testthat&lt;/code&gt; looks for files that start with &lt;code&gt;test_&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Once your file is set up, we can start writing tests! First, you source in the file that you’ll be testing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;source(&amp;#39;dummy_script.R&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first thing I’ll be testing is that the data I defined in the dummy script meets my expectations. For reference, this is the data file I defined, but we can pretend I read it in from somewhere:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;testing_data &amp;lt;- data.frame(&amp;#39;letters&amp;#39;=c(&amp;#39;a&amp;#39;, &amp;#39;b&amp;#39;, &amp;#39;c&amp;#39;, &amp;#39;d&amp;#39;),
                          &amp;#39;numbers&amp;#39;=seq(1, 4))
print(testing_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   letters numbers
## 1       a       1
## 2       b       2
## 3       c       3
## 4       d       4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Creative, huh?&lt;/p&gt;
&lt;p&gt;So, because this dummy script has multiple different parts that will all get tested, I’m first going to set a context just for the data. Doing so couldn’t be easier:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;context(&amp;#39;testing data integrity&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This just gives the test block a readable label that prints during output. Typically you have one context per file, but I’ve got multiple in this file for demo purposes.&lt;/p&gt;
&lt;p&gt;Within a context, you have tests. Tests are best deployed to test very targeted functionality, and each test contains a small number of expect statements that define expected behavior. Here’s an example with our silly data frame. We want to make sure it has the expected number of columns and rows, so we could write a test that checks for this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_that(&amp;#39;data dimensions correct&amp;#39;, {
    expect_equal(ncol(testing_data), 2)
    expect_equal(nrow(testing_data), 4)
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All we’re saying is we expect the actual number of columns in our data, returned by ncol(), to be equal to our number of variables (here 2), and that we should have four subjects. The first argument to &lt;code&gt;expect_equal&lt;/code&gt; is what we’re testing, and the second argument is what that thing should be equal to (both of these tests pass).&lt;/p&gt;
&lt;p&gt;This is sort of a goofy example, but you can imagine real applications–making sure your data has the right number of variables, for instance, or making sure that you have as many files as you do subjects to make sure data isn’t missing. If you’ve examined your data up using &lt;code&gt;View()&lt;/code&gt; or &lt;code&gt;head()&lt;/code&gt; just to check, you’ve pretty much done the manual equivalent of this test.&lt;/p&gt;
&lt;p&gt;We could also check that no values are missing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_that(&amp;#39;no missing values&amp;#39;, {
    expect_identical(testing_data, na.omit(testing_data))
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a different expect statement. This is &lt;code&gt;expect_identical&lt;/code&gt;, which has no tolerance (unlike &lt;code&gt;expect_equal&lt;/code&gt;). We should have exactly the same thing after removing missing values if we’re not anticipating any. (There’s also an &lt;code&gt;expect_equivalent&lt;/code&gt; statement, which ignores the attributes of its comparators. There are about 22 different &lt;code&gt;expect_&lt;/code&gt; statements–you can read more about them in the &lt;a href=&#34;http://r-pkgs.had.co.nz/tests.html&#34;&gt;package documentation&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;How about we check to make sure our data is of the right type? We can use &lt;code&gt;expect_is()&lt;/code&gt;, which checks if the first argument inherits from the class specified in the second argument.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_that(&amp;#39;data types correct&amp;#39;, {
    expect_is(testing_data,&amp;#39;data.frame&amp;#39;)
    expect_is(testing_data$numbers, &amp;#39;integer&amp;#39;)
    #expect_is(testing_data$letters, &amp;#39;character&amp;#39;) #this one fails; they&amp;#39;re factors
})

#note that an equivalent statement would be:

expect_that(testing_data, is_a(&amp;#39;data.frame&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This might seem silly, but I’ve been burned before by not realizing that what I thought was numeric data was actually stored as a factor. Never hurts to check before you start analyzing!&lt;/p&gt;
&lt;p&gt;We can also, for example, run tests to make sure outputs from models conform to expectations. Here’s our toy model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_data = data.frame(&amp;#39;y&amp;#39;=c(rnorm(25, 0, 1), rnorm(25, 1, 1)),
&amp;#39;x&amp;#39;=rep(c(&amp;#39;c1&amp;#39;, &amp;#39;c2&amp;#39;), each=25))
test.mod = lm(y ~ x, data=model_data)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we could test, for example, that we have the expected number of coefficients:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_that(&amp;#39;right number of coefficients&amp;#39;, {
    expect_equal(length(test.mod$coefficients), 2)
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or that all the factor levels in the data are also in the model (here we have to be mindful about differences in data type; &lt;code&gt;levels&lt;/code&gt; returns a character string, while the model object returns a named list).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_that(&amp;#39;all factor levels present&amp;#39;, {
    expect_equivalent(levels(model_data$x), unlist(test.mod$xlevels))
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How about verifying that the intercept equals the mean of our first group? Because one value is named and the other isn’t, &lt;code&gt;expect_equivalent&lt;/code&gt; will be the statement to use:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_that(&amp;#39;mean of group 1 equals intercept&amp;#39;, {
    expect_equivalent(mean(model_data$y[model_data$x == &amp;#39;c1&amp;#39;]), test.mod$coefficients[&amp;#39;(Intercept)&amp;#39;])
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a very simple model, but you can imagine how it would be useful for a more complicated one.&lt;/p&gt;
&lt;p&gt;We can also test custom functions that we’ve written. I’ve written a silly little function that tells you whether a number is even or odd:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;even_odd = function(n){
    ifelse(n %% 2 == 0, print(&amp;#39;even&amp;#39;), print(&amp;#39;odd&amp;#39;))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we can make sure it reports the correct output. We can do this even though it prints (instead of returning a value) with prints_text:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_that(&amp;#39;even_odd prints the right message&amp;#39;, {
    expect_that(even_odd(1), prints_text(&amp;#39;odd&amp;#39;))
    expect_that(even_odd(6), prints_text(&amp;#39;even&amp;#39;))
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After you have the library loaded in and you’ve set the working directory to the right place, you can run the testing suite by calling &lt;code&gt;test_file(&#39;file_to_test.R&#39;)&lt;/code&gt;. The advantage to calling this function, rather than just sourcing the file, is that it will run through all tests even if some fail. When you source it, the test script halts at the first error. If you have multiple testing files, you can call &lt;code&gt;test_dir(&#39;my_test_dir/&#39;)&lt;/code&gt; instead, and it will run all of the &lt;code&gt;test_&lt;/code&gt; files in that directory.&lt;/p&gt;
&lt;p&gt;Here’s what output looks like. You can see that all of our tests are grouped under their contexts, which is a nice way to organize things. Each little dot is a successful test, while a number indicates a failure (with more detail printed below).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; test_file(&amp;#39;test_dummy.R&amp;#39;)
testing data integrity: .....1
testing model output: ...
testing a custom function: ..&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we look in detail at the failure, we see that it’s titled with the name of our test_that suite (another good reason to be granular with how you organize your tests!), and then we see what went wrong:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Failed ---------------------------------------------------------------------------------------------
1. Failure: data types correct (@test_dummy.R#17) -----------------------------------------------------
testing_data$letters inherits from `factor` not `character`.


DONE ===============================================================================================&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the context tells us which group of tests threw the error, and then the more detailed error message tells us which test failed (“data types correct” in the “data integrity” context). It also tells us exactly why the test failed. We wanted the data type to be a character, but it was read in as a factor. This alerts us that we need to set &lt;code&gt;stringsAsFactors&lt;/code&gt; equal to &lt;code&gt;FALSE&lt;/code&gt; when reading in our data (for example).&lt;/p&gt;
&lt;p&gt;Here are some general tips on writing tests from Wickham:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Focus on testing the external interface to your functions - if you test the internal interface, then it’s harder to change the implementation in the future because as well as modifying the code, you’ll also need to update all the tests.&lt;br /&gt;
Strive to test each behaviour in one and only one test. Then if that behaviour later changes you only need to update a single test.&lt;br /&gt;
Avoid testing simple code that you’re confident will work. Instead focus your time on code that you’re not sure about, is fragile, or has complicated interdependencies. That said, I often find I make the most mistakes when I falsely assume that the problem is simple and doesn’t need any tests. Always write a test when you discover a bug. You may find it helpful to adopt the test-first philosophy. There you always start by writing the tests, and then write the code that makes them pass. This reflects an important problem solving strategy: start by establishing your success criteria, how you know if you’ve solved the problem. The nice thing about testing is that you can re-run the entire, automated set of tests any time you make a change to make sure you didn’t break anything. It’s a lot faster and more consistent than print-debugging or command-line inspection, and it will save you time if you write your tests early (or even before) in the coding process.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Happy testing!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Intro to R Notebooks</title>
      <link>https://katherinemwood.github.io/post/notebooks/</link>
      <pubDate>Mon, 26 Jun 2017 15:36:22 -0500</pubDate>
      
      <guid>https://katherinemwood.github.io/post/notebooks/</guid>
      <description>&lt;p&gt;This notebook is based loosely on &lt;a href=&#34;https://www.rstudio.com/wp-content/uploads/2016/03/rmarkdown-cheatsheet-2.0.pdf&#34;&gt;this cheatsheet&lt;/a&gt; that shows some of the most basic functionality of R notebooks, including code, markdown, and Latex.&lt;/p&gt;
&lt;div id=&#34;code&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Code&lt;/h1&gt;
&lt;div id=&#34;chunks&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Chunks&lt;/h2&gt;
&lt;p&gt;You can imbed chunks of R code right into the notebooks and run them. Why not beat that long-dead horse of demo data, the iris dataset?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(iris)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
## 2          4.9         3.0          1.4         0.2  setosa
## 3          4.7         3.2          1.3         0.2  setosa
## 4          4.6         3.1          1.5         0.2  setosa
## 5          5.0         3.6          1.4         0.2  setosa
## 6          5.4         3.9          1.7         0.4  setosa&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you print a dataframe, it automatically gets formatted in this nice way.&lt;/p&gt;
&lt;p&gt;We can also look at the summary:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(iris)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   
##  Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  
##  1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  
##  Median :5.800   Median :3.000   Median :4.350   Median :1.300  
##  Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  
##  3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  
##  Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  
##        Species  
##  setosa    :50  
##  versicolor:50  
##  virginica :50  
##                 
##                 
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And of course, you can use whatever libraries you like, as usual. We can set an option in this chunk to disable the messages that print to the console when you do things like load libraries. We could also turn off warnings, set options for error handling, or specify how we want results and code interleaved, or if we want the code in the output document at all. Setting the &lt;code&gt;tidy&lt;/code&gt; argument to &lt;code&gt;TRUE&lt;/code&gt; also spiffs up your code for you (cleaning up multiline expressions with indenting, for instance) in the output. So customizable!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
(meanpetals &amp;lt;- group_by(iris, Species) %&amp;gt;%
               summarize(length = mean(Petal.Length), width = mean(Petal.Width)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 3
##      Species length width
##       &amp;lt;fctr&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1     setosa  1.462 0.246
## 2 versicolor  4.260 1.326
## 3  virginica  5.552 2.026&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Boring old data demos never looked so fresh!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;inline&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Inline&lt;/h2&gt;
&lt;p&gt;Inline code executes invisibly, so all you see is the output. For example, I can run a function right here and now to get the weekday:&lt;/p&gt;
&lt;p&gt;Today is a Sunday.&lt;/p&gt;
&lt;p&gt;That was accomplished by simply adding a back-ticked &lt;code&gt;r&lt;/code&gt; prefix to a call to &lt;code&gt;weekdays(Sys.time())&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;You can also set global parameters in the header and reference them throughout the document.&lt;/p&gt;
&lt;p&gt;This inline execution has various uses. If you need code primarily for its output, like getting the current date or weekday and adding it into text, then inline works really well and better than a chunk.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;displaying-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Displaying Data&lt;/h1&gt;
&lt;p&gt;If you’re writing up a tutorial or a report on results, you’ll more likely than not need to display data in the form of plots and tables. This is super easy in R Notebooks, and there are lots of options.&lt;/p&gt;
&lt;div id=&#34;plots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plots&lt;/h2&gt;
&lt;p&gt;Let’s see what the sepal lengths in Iris are up to by species, just for kicks. I’ll do some more extensive data manipulation in a code chunk, just to show we can. Since I loaded dplyr in another code chunk further up, we’re good there.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
iris_plot &amp;lt;- ggplot(data=select(iris, Species, Sepal.Length), 
                    aes(x=Species, y=Sepal.Length, color=Species, fill=Species)) +
  geom_violin(alpha=.1) +
  geom_point(position=position_jitter(w=.2)) +
  geom_crossbar(stat=&amp;#39;summary&amp;#39;,fun.y=mean, fun.ymax=mean, fun.ymin=mean, fatten=2, width=.5) +
  theme_minimal()
print(iris_plot)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../&#34;katherinemwood.github.io/&#34;  # End your URL with a `/` trailing slash.post/notebooks_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Fascinating, I guess!&lt;/p&gt;
&lt;p&gt;The nice thing about this is that these plots are completely reproduceable–they’ll be generated everytime you run the notebook. You can keep the code cell that generates it in the output, so people can see it, or if you want a cleaner output with just the results, you can hide the generating code cell. You can tailor it to your options.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tables&lt;/h2&gt;
&lt;p&gt;Tables can be a real pain-point when doing manuscripts, but these can be automated in R Notebooks, rendered right into text, and some look pretty good. Here’s what &lt;code&gt;knitr&lt;/code&gt; will give you (it won’t look right until it’s rendered):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable(meanpetals)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Species&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;length&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;width&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;setosa&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.462&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.246&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;versicolor&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.260&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.326&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;virginica&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.552&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.026&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;That looks alright!&lt;/p&gt;
&lt;p&gt;There’s also &lt;code&gt;xtable,&lt;/code&gt; which will make it easier to set the appearance of the table through HTML attributes. This won’t show up until it’s rendered, though.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(xtable::xtable(meanpetals), type=&amp;#39;html&amp;#39;, html.table.attributes=&amp;#39;border=0  width=250&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;!-- html table generated in R 3.4.1 by xtable 1.8-2 package --&gt;
&lt;!-- Sun Oct  8 22:20:39 2017 --&gt;
&lt;table border=&#34;0&#34; width=&#34;250&#34;&gt;
&lt;tr&gt;
&lt;th&gt;
&lt;/th&gt;
&lt;th&gt;
Species
&lt;/th&gt;
&lt;th&gt;
length
&lt;/th&gt;
&lt;th&gt;
width
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;
1
&lt;/td&gt;
&lt;td&gt;
setosa
&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
1.46
&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
0.25
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;
2
&lt;/td&gt;
&lt;td&gt;
versicolor
&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
4.26
&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
1.33
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;
3
&lt;/td&gt;
&lt;td&gt;
virginica
&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
5.55
&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
2.03
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Finally, here’s a table fron the &lt;code&gt;stargazer&lt;/code&gt; package. Also lots of attributes we can set here. It’s designed mostly for pretty formatting of regression model results and summary stats, but we can get it to do direct output instead.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stargazer::stargazer(as.data.frame(meanpetals), type=&amp;#39;html&amp;#39;, summary=F)&lt;/code&gt;&lt;/pre&gt;
&lt;table style=&#34;text-align:center&#34;&gt;
&lt;tr&gt;
&lt;td colspan=&#34;4&#34; style=&#34;border-bottom: 1px solid black&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;
&lt;/td&gt;
&lt;td&gt;
Species
&lt;/td&gt;
&lt;td&gt;
length
&lt;/td&gt;
&lt;td&gt;
width
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan=&#34;4&#34; style=&#34;border-bottom: 1px solid black&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;
1
&lt;/td&gt;
&lt;td&gt;
setosa
&lt;/td&gt;
&lt;td&gt;
1.462
&lt;/td&gt;
&lt;td&gt;
0.246
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;
2
&lt;/td&gt;
&lt;td&gt;
versicolor
&lt;/td&gt;
&lt;td&gt;
4.260
&lt;/td&gt;
&lt;td&gt;
1.326
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;
3
&lt;/td&gt;
&lt;td&gt;
virginica
&lt;/td&gt;
&lt;td&gt;
5.552
&lt;/td&gt;
&lt;td&gt;
2.026
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan=&#34;4&#34; style=&#34;border-bottom: 1px solid black&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;If none of the formatting is quite to your liking, or you don’t have easily available options to set, you can tweak the appearance with your &lt;a href=&#34;http://rmarkdown.rstudio.com/html_document_format.html#custom_css&#34;&gt;own CSS&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If none of this is satisfying, or it’s more informal, you can always just print the dataframe.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(meanpetals)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 3
##      Species length width
##       &amp;lt;fctr&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1     setosa  1.462 0.246
## 2 versicolor  4.260 1.326
## 3  virginica  5.552 2.026&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I don’t know that this makes tables any less painless, but at least now there’s more than one option for the pain.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;formatting&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Formatting&lt;/h1&gt;
&lt;p&gt;If you’re writing up a report or manuscript, you also need your text to look nice, not just your data. This is where knowing a little bit of Markdown comes in handy.&lt;/p&gt;
&lt;div id=&#34;markdown&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Markdown&lt;/h2&gt;
&lt;p&gt;Just type to get pretty plaintext. To quote &lt;code&gt;verbatim code&lt;/code&gt; without running anything, wrap it in backticks(`). This is distinct from inline code, which is prefaced with &lt;code&gt;r&lt;/code&gt; within the ticks. I’ll do this throughout to demonstrate how to construct the effects you see.&lt;/p&gt;
&lt;p&gt;To display a special character, like _underscores_ or asterisks *, escape them with a backslash (\).&lt;/p&gt;
&lt;p&gt;To add a single linebreak without a blank line, end a line with two spaces.&lt;code&gt;..&lt;/code&gt;&lt;br /&gt;
Ta-da!&lt;/p&gt;
&lt;p&gt;&lt;code&gt;*italics*&lt;/code&gt;: &lt;em&gt;italics&lt;/em&gt;&lt;br /&gt;
&lt;code&gt;**bold**&lt;/code&gt;: &lt;strong&gt;bold&lt;/strong&gt;&lt;br /&gt;
&lt;code&gt;**_both_**&lt;/code&gt;: &lt;strong&gt;&lt;em&gt;both&lt;/em&gt;&lt;/strong&gt;&lt;br /&gt;
&lt;code&gt;~~strikethrough~~&lt;/code&gt;: &lt;del&gt;strikethrough&lt;/del&gt;&lt;br /&gt;
&lt;code&gt;subscript~1~&lt;/code&gt;: subscript&lt;sub&gt;1&lt;/sub&gt;&lt;br /&gt;
&lt;code&gt;superscript^2^&lt;/code&gt;: superscript&lt;sup&gt;2&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;You can do bulleted lists:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;* start with an asterisk, plus, or minus&lt;/code&gt; &lt;code&gt;+ then indent 4 spaces&lt;/code&gt; &lt;code&gt;- then indent again&lt;/code&gt; &lt;code&gt;* then more stuff&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;start with an asterisk, plus, or minus
&lt;ul&gt;
&lt;li&gt;then indent 4 spaces
&lt;ul&gt;
&lt;li&gt;then indent again&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;then more stuff&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And ordered ones:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;1. Big thing&lt;/code&gt; &lt;code&gt;i) small thing \+ indent&lt;/code&gt; &lt;code&gt;A. sub-small thing&lt;/code&gt; &lt;code&gt;2. Another big thing&lt;/code&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Big thing
&lt;ol style=&#34;list-style-type: lower-roman&#34;&gt;
&lt;li&gt;small thing + indent&lt;br /&gt;
A. sub-small thing&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;Another big thing&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Oh, and footnotes… &lt;code&gt;[^1]&lt;/code&gt; &lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;#Header 1&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;header-1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Header 1&lt;/h1&gt;
&lt;p&gt;&lt;code&gt;##Header 2&lt;/code&gt;&lt;/p&gt;
&lt;div id=&#34;header-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Header 2&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;###Header 3&lt;/code&gt;&lt;/p&gt;
&lt;div id=&#34;header-3&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Header 3&lt;/h3&gt;
&lt;p&gt;Add horizontal lines with at least three hyphens, asterisks, or underscores:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;***&lt;/code&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Tables are a little strange in the raw, but look nice once rendered.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;| Right-aligned | Left-aligned | Centered | Default |
|-:|:-|:-:|-|
|1|1|1|1|
|12|12|12|12| 
|*1*|_2_|~~3~~|4^2^|&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;Right-aligned&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Left-aligned&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Centered&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;12&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;&lt;em&gt;1&lt;/em&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;em&gt;2&lt;/em&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;del&gt;3&lt;/del&gt;&lt;/td&gt;
&lt;td&gt;4&lt;sup&gt;2&lt;/sup&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Images are easy, too.&lt;br /&gt;
&lt;code&gt;![The adorabilis octopus, for your viewing pleasure](http://blogs.discovermagazine.com/d-brief/files/2015/06/octopus.jpg)&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;http://blogs.discovermagazine.com/d-brief/files/2015/06/octopus.jpg&#34; alt=&#34;The adorabilis octopus, for your viewing pleasure&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;The adorabilis octopus, for your viewing pleasure&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;code&gt;[^1]:&lt;/code&gt; Are pretty easy.&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Data in the raw: Violin plots</title>
      <link>https://katherinemwood.github.io/post/violins/</link>
      <pubDate>Mon, 26 Jun 2017 15:27:06 -0500</pubDate>
      
      <guid>https://katherinemwood.github.io/post/violins/</guid>
      <description>&lt;p&gt;One of the ways we can increase transparency in science, in addition to posting our data, materials, and pre-registering our methods, is to start including more information about our raw data in our write-ups and reports. One of the ways we can do this is just show it in a visualization.&lt;/p&gt;
&lt;p&gt;The bar chart with error bars (usually &lt;span class=&#34;math inline&#34;&gt;\(\pm 1.96 \times\)&lt;/span&gt; standard error for a 95% confidence interval) is a classic plot type, but it obscures a lot of information about the underlying distribution that generated it. Scatterplots and histograms show more of the raw distribution, but they can be messy and hard to cleanly overlay with summary statistics.&lt;/p&gt;
&lt;p&gt;Fortunately, violin plots bring together the informativeness of a histogram with the cleanliness of a bar chart, and they can be easily overlaid with summary statistics, error bars, and other information without too much additional clutter.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(dplyr)

set.seed(836)
dat &amp;lt;- data.frame(&amp;#39;condition&amp;#39;=c(rep(&amp;#39;t1&amp;#39;, 30), rep(&amp;#39;t2&amp;#39;, 30)), 
                  &amp;#39;value&amp;#39;=c(rnorm(30, 10, 3), rnorm(30, 20, 7)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the simplest incarnation of a violin plot, for two normally distributed groups:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;basic_violin &amp;lt;- ggplot(data=dat, aes(x=condition, y=value)) +
  geom_violin(aes(fill=condition, color=condition)) +
  theme_minimal()

print(basic_violin)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../&#34;katherinemwood.github.io/&#34;  # End your URL with a `/` trailing slash.post/violins_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;These might be more accurately called “jug” or “vase” plots, since they rarely make pretty violin shapes and more often tend to look like postmodern sculpture. Name aside, the violin plot is a rotated, symmetric kernel density plot that shows the density of points at different values. Where the plot is wide, there is a high density of points; where it is narrow, a low density of points (like height on a histogram). We can see here that group T1 is less variable than T2; T1’s violin is short and squat, meaning most of the points are massed in a small region. T2, on the other hand, is tall and narrow, meaning the points are spread thinner along a wider range of values&lt;/p&gt;
&lt;p&gt;These guys looks a little sparse, though. Why don’t we dress them up a smidge?&lt;/p&gt;
&lt;p&gt;Maybe you like a point mean and &lt;span class=&#34;math inline&#34;&gt;\(2*\)&lt;/span&gt;SE error bars:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;errbar_lims &amp;lt;- group_by(dat, condition) %&amp;gt;% 
              summarize(mean=mean(value), se=sd(value)/sqrt(n()), 
                        upper=mean+(2*se), lower=mean-(2*se))

mean_se_violin &amp;lt;- ggplot() +
  geom_violin(data=dat, aes(x=condition, y=value, fill=condition, color=condition)) +
  geom_point(data=errbar_lims, aes(x=condition, y=mean), size=3) +
  geom_errorbar(aes(x=errbar_lims$condition, ymax=errbar_lims$upper, 
                ymin=errbar_lims$lower), stat=&amp;#39;identity&amp;#39;, width=.25) +
  theme_minimal()

print(mean_se_violin)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../&#34;katherinemwood.github.io/&#34;  # End your URL with a `/` trailing slash.post/violins_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Or maybe boxplots are your jam:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;boxplot_violin &amp;lt;- ggplot(data=dat, aes(x=condition, y=value)) +
  geom_violin(aes(fill=condition, color=condition)) +
  geom_boxplot(width=.1, outlier.shape=NA) +
  theme_minimal()

print(boxplot_violin)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../&#34;katherinemwood.github.io/&#34;  # End your URL with a `/` trailing slash.post/violins_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here’s the way I like to do my violin plots, with each subject’s point plotted plus a horizontal line for the mean. I add a little horizontal jitter to each point to make things easier to see:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;scatter_violin &amp;lt;- ggplot(data=dat, aes(x=condition, y=value)) +
  geom_violin(aes(fill=condition, color=condition)) +
  geom_crossbar(stat=&amp;quot;summary&amp;quot;, fun.y=mean, fun.ymax=mean, fun.ymin=mean, fatten=2, width=.5) +
  geom_point(color=&amp;quot;black&amp;quot;, size=1, position = position_jitter(w=0.05)) +
  theme_minimal()

print(scatter_violin)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../&#34;katherinemwood.github.io/&#34;  # End your URL with a `/` trailing slash.post/violins_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are a few things I really like about violin plots. One, they add a lot more information without taking up any more space than a bar plot would. Two, they give you an intuitive way to look at the distributions of your data. I don’t think many of us are accustomed to seeing data this way, and it goes to show that even well-behaved data doesn’t necessarily look like we might expect. &lt;code&gt;t1&lt;/code&gt; is just as normal as &lt;code&gt;t2&lt;/code&gt;, but &lt;code&gt;t1&lt;/code&gt; “looks” a lot more normal than &lt;code&gt;t2&lt;/code&gt; does. You lose a lot of the characteristics of the data with a bar plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bar_plot &amp;lt;- ggplot(data=errbar_lims) +
  geom_bar(aes(x=condition, y=mean, fill=condition, color=condition), stat=&amp;#39;identity&amp;#39;, position=&amp;#39;dodge&amp;#39;) +
  geom_errorbar(aes(x=condition, ymax=upper, 
                    ymin=lower), stat=&amp;#39;identity&amp;#39;, width=.25) +
  theme_minimal()

print(bar_plot)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../&#34;katherinemwood.github.io/&#34;  # End your URL with a `/` trailing slash.post/violins_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It captures the broad strokes, to be sure, but a lot of the finer details disappear.&lt;/p&gt;
&lt;p&gt;So there you go. Violin plots! Dress ’em up, dress ’em down. However you like them, they’re a nice plot type to have in your arsenal.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Primer on Data Wrangling</title>
      <link>https://katherinemwood.github.io/post/wrangling/</link>
      <pubDate>Mon, 26 Jun 2017 15:20:35 -0500</pubDate>
      
      <guid>https://katherinemwood.github.io/post/wrangling/</guid>
      <description>&lt;p&gt;I found &lt;a href=&#34;https://twitter.com/dalejbarr/status/826717889608765444&#34;&gt;this Twitter thread&lt;/a&gt; on the vagaries of data wrangling killing momentum interesting, particularly the notion of how frustrating it must be to be at point A with your data, see Point B, where you’d like to go, and have no idea how to get there. Even for those with programming experience, data wrangling can be an enormous chore.&lt;/p&gt;
&lt;p&gt;To that end, I thought I’d walk through a basic overview of how to accomplish some of the operations you might commonly encounter when you first get a data set. If you’re generating the data yourself, you can try to make your life easier by saving it in the format you want.&lt;/p&gt;
&lt;p&gt;Where possible, I’ll show multiple ways to accomplish something and try to highlight packages that will make things easier.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;MANDATORY DISCLAIMER:&lt;/strong&gt; There are often at least six ways to do anything in R. If you don’t like any of the methods here, rest assured that there are others (and probably better ones, too); you can almost certainly find something that suits your style.&lt;/p&gt;
&lt;div id=&#34;reading-in-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Reading in data&lt;/h1&gt;
&lt;div id=&#34;csv-files&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;CSV Files&lt;/h3&gt;
&lt;p&gt;First, I’m going to create some data to actually read in. The following code chunk will write three csv files to the current directory, each with 3 columns of random data. This is meant to simulate raw data from three different subjects.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Generate some dummy data
data &amp;lt;- replicate(3, mapply(rnorm, c(100, 100, 100),  c(10, 100, 1), c(2, 25, .5)), 
                  simplify = FALSE)
catch_output &amp;lt;- mapply(write.csv, data,
       mapply(paste, rep(&amp;quot;data&amp;quot;, times=length(data)),
              seq(1, length(data)),
              rep(&amp;quot;.csv&amp;quot;), sep=&amp;quot;&amp;quot;), row.names=FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s a common situation: one spreadsheet has all of a subject’s raw data, and you have a few dozen spreadsheets. First, let’s talk about an easy way to read that in as painlessly as possible. No for-loops needed here; we’ll just use the trusty &lt;code&gt;apply()&lt;/code&gt; family from base R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Note that if your data are not in your current directory, you need to either:
#Call, for ex., setwd(&amp;#39;~/my_data_folder&amp;#39;) to set the data folder as the current directory
#Specify the path in list.files and then have it return the full path name of each file, rather than #the relative path.
alldata &amp;lt;- lapply(list.files(pattern = &amp;#39;*.csv&amp;#39;), read.csv)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’re accomplishing a few things in one line. First, the call to &lt;code&gt;list.files&lt;/code&gt; simply lists all of the files in your current directory. It has a bunch of optional arguments, too. You can specify a pattern, which is just a regex expression that specifies what you want. Here, I only want .csv files, so I specify that I want any file (“*&amp;quot; is a wildcard symbol allowing anything) that ends in the extension .csv. I can specify other arguments, like whether I want the full path names returned, whether I want it to also search sub-directories for files, and so on.&lt;/p&gt;
&lt;p&gt;After we have this list of files, we simply iterate over it and call &lt;code&gt;read.csv&lt;/code&gt; on each one. The end result is a list, wherein each element is one subject’s data frame.&lt;/p&gt;
&lt;p&gt;Now, a list of data frames is not the &lt;em&gt;most&lt;/em&gt; useful data format to have. Fortunately, it’s easy to bind this list together into one big data frame. Here’s how to bring it all together in base R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subjects_all &amp;lt;- do.call(&amp;#39;rbind&amp;#39;, alldata)
head(subjects_all)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          V1        V2        V3
## 1  8.949957 124.80946 1.3867713
## 2 11.926203 117.26313 1.0590077
## 3  6.922328  78.01893 1.1814264
## 4  9.323430  91.66125 0.5276688
## 5 11.648274 140.93698 1.6975092
## 6  6.048526  65.44654 1.3584207&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the information about which data belong to each subject is lost here. You’ll need to add an identifier column, or make sure that each file has one, before reading it in this way.&lt;/p&gt;
&lt;p&gt;How about some good old dplyr?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
subjects_all &amp;lt;- bind_rows(alldata, .id=&amp;#39;subject&amp;#39;)
head(subjects_all)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   subject        V1        V2        V3
## 1       1  8.949957 124.80946 1.3867713
## 2       1 11.926203 117.26313 1.0590077
## 3       1  6.922328  78.01893 1.1814264
## 4       1  9.323430  91.66125 0.5276688
## 5       1 11.648274 140.93698 1.6975092
## 6       1  6.048526  65.44654 1.3584207&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;unique(subjects_all$subject)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;1&amp;quot; &amp;quot;2&amp;quot; &amp;quot;3&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can use the&lt;code&gt;.id&lt;/code&gt; argument to specify an ID column, which will keep track of where the data comes from.&lt;/p&gt;
&lt;p&gt;We can also use the handy rbindlist function from the &lt;a href=&#34;https://cran.r-project.org/web/packages/data.table/data.table.pdf&#34;&gt;data.table&lt;/a&gt;/&lt;a href=&#34;https://cran.r-project.org/web/packages/dtplyr/dtplyr.pdf&#34;&gt;dtplyr&lt;/a&gt; package. This will label the data automatically for us according to which data frame it came from; we can call this new column (specified by the id argument) anything we like.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(data.table)
subjects_all &amp;lt;- rbindlist(alldata, idcol=&amp;#39;subject&amp;#39;)
head(subjects_all)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    subject        V1        V2        V3
## 1:       1  8.949957 124.80946 1.3867713
## 2:       1 11.926203 117.26313 1.0590077
## 3:       1  6.922328  78.01893 1.1814264
## 4:       1  9.323430  91.66125 0.5276688
## 5:       1 11.648274 140.93698 1.6975092
## 6:       1  6.048526  65.44654 1.3584207&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;unique(subjects_all$subject)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 2 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note also that &lt;code&gt;rbindlist()&lt;/code&gt; is an order of magnitude faster than &lt;code&gt;do.call&lt;/code&gt;. If you’ve got a lot of data, you’ll probably want to go with this function. &lt;code&gt;data.tables&lt;/code&gt; are extremely fast and memory efficient in general, and might be a good option if you’re working with truly huge amounts of data. For most uses, though, this kind of optimization isn’t really necessary.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;text-files&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Text files&lt;/h3&gt;
&lt;p&gt;Same process. I’ll make some real quick:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;catch_output &amp;lt;- mapply(write.table, data,
       mapply(paste, rep(&amp;quot;data&amp;quot;, times=length(data)),
              seq(1, length(data)),
              rep(&amp;quot;.txt&amp;quot;), sep=&amp;quot;&amp;quot;), row.names=FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To read it in, we just call &lt;code&gt;read.table&lt;/code&gt; instead.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;allsubjs &amp;lt;- lapply(list.files(pattern = &amp;#39;*.txt&amp;#39;), read.table, header=TRUE, colClasses=c(&amp;#39;double&amp;#39;))
head(allsubjs[[1]])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          V1        V2        V3
## 1  8.949957 124.80946 1.3867713
## 2 11.926203 117.26313 1.0590077
## 3  6.922328  78.01893 1.1814264
## 4  9.323430  91.66125 0.5276688
## 5 11.648274 140.93698 1.6975092
## 6  6.048526  65.44654 1.3584207&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Just like before, we end up with a list of data frames, one for each subject. In &lt;code&gt;read.table()&lt;/code&gt;, unlike &lt;code&gt;read.csv&lt;/code&gt;, the &lt;code&gt;header&lt;/code&gt; argument defaults to &lt;code&gt;FALSE&lt;/code&gt;, so be sure to change that. I also specify &lt;code&gt;colClasses&lt;/code&gt; here to tell R what type of data the content of the columns is. Without that, these doubles get read in as factors; doing it now saves a little work later.&lt;/p&gt;
&lt;p&gt;We can then bind these together with &lt;code&gt;rbindlist&lt;/code&gt; just like we did when we used &lt;code&gt;read.csv&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;xlsx&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;XLS(X)&lt;/h3&gt;
&lt;p&gt;Need to read in Excel files, or read in each sheet in one file as a separate set of data?&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/web/packages/readxl/readxl.pdf&#34;&gt;Jenny Bryan et al. to the rescue.&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-general-notes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Some general notes&lt;/h3&gt;
&lt;p&gt;There are a lot of arguments you can specify in the &lt;code&gt;read.csv&lt;/code&gt; function call that can save you work down the line–I used some of them when I was reading in the text files, but there are many more. You can even tell the function what strings should be read as NA values! This is really handy if you have NULL and what R to treat that as NA. You can also read in only part of the file, which is useful if you have a monster file and want to read it in in chunks.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;reshaping-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Reshaping Data&lt;/h1&gt;
&lt;p&gt;It’s helpful to be able to switch at will between data in wide format, where each row is a subject and each column contains a variable, to long format, where each row is a value at a given time, or measure (for repeated measures).&lt;/p&gt;
&lt;p&gt;Here’s a very simple data set. Each row is a subject’s scores. Column 1 is their subject number, followed by their scores in the control, treatment 1, and treatment 2 conditions. We tend to be most accustomed to seeing data this way. This is “wide” format.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;traits &amp;lt;- data.frame(&amp;#39;id&amp;#39;=seq(1, 10),
                     &amp;#39;control&amp;#39;=floor(rnorm(10, 30, 5)),
                     &amp;#39;treat1&amp;#39;=floor(rnorm(10, 10, 2)),
                     &amp;#39;treat2&amp;#39;=floor(rnorm(10, 15, 3)))
print(traits)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    id control treat1 treat2
## 1   1      36      8     21
## 2   2      33     11      9
## 3   3      27      9     13
## 4   4      26      6     14
## 5   5      23      9      8
## 6   6      33     12     13
## 7   7      30      9     19
## 8   8      29     13     11
## 9   9      22     12     11
## 10 10      23     10     16&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;wide-to-long&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Wide to Long&lt;/h3&gt;
&lt;p&gt;Now, when we cast this to “long” format, we will have the id column (the subject number), a variable column (in this case, which test was taken), and the value column (the score on each test). Here it is, melted two ways. In base:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;traits_long_base &amp;lt;- reshape(traits, idvar=&amp;quot;id&amp;quot;, direction=&amp;#39;long&amp;#39;, v.names=c(&amp;#39;score&amp;#39;),
                       timevar=&amp;#39;test&amp;#39;,times=c(&amp;#39;control&amp;#39;, &amp;#39;treat1&amp;#39;, &amp;#39;treat2&amp;#39;), varying=seq(2, 4))
print(traits_long_base)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            id    test score
## 1.control   1 control    36
## 2.control   2 control    33
## 3.control   3 control    27
## 4.control   4 control    26
## 5.control   5 control    23
## 6.control   6 control    33
## 7.control   7 control    30
## 8.control   8 control    29
## 9.control   9 control    22
## 10.control 10 control    23
## 1.treat1    1  treat1     8
## 2.treat1    2  treat1    11
## 3.treat1    3  treat1     9
## 4.treat1    4  treat1     6
## 5.treat1    5  treat1     9
## 6.treat1    6  treat1    12
## 7.treat1    7  treat1     9
## 8.treat1    8  treat1    13
## 9.treat1    9  treat1    12
## 10.treat1  10  treat1    10
## 1.treat2    1  treat2    21
## 2.treat2    2  treat2     9
## 3.treat2    3  treat2    13
## 4.treat2    4  treat2    14
## 5.treat2    5  treat2     8
## 6.treat2    6  treat2    13
## 7.treat2    7  treat2    19
## 8.treat2    8  treat2    11
## 9.treat2    9  treat2    11
## 10.treat2  10  treat2    16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have to be pretty careful about specifying our arguments here. The &lt;code&gt;idvar&lt;/code&gt; indicates which column we want to map over the data. Here, we want the subject number; we want each subject’s score on each test labeled with their unique ID. Direction is fairly self-explanatory; we’re going to long form here. &lt;code&gt;v.names&lt;/code&gt; is the name (or names) of the new columns. Here, we’re collapsing everybody’s scores into a single column, so we call it &lt;code&gt;&#39;score&#39;&lt;/code&gt;. &lt;code&gt;timevar&lt;/code&gt; is the variable that changes over time, or over repeated measures. Here it’s which test they took, so we call the new column &lt;code&gt;&#39;test&#39;&lt;/code&gt;. Then we tell it which values to use in this new times column with &lt;code&gt;times&lt;/code&gt;; we want the name of the test. Then we tell it which columns of our data are varying over time/are our repeated measures; here it’s the final three columns (you can also specify a vector of strings).&lt;/p&gt;
&lt;p&gt;Here are the same results with the &lt;code&gt;melt()&lt;/code&gt; function from data.table or reshape2. We specify again which column represents our data labels, and then we tell it which columns we want it to treat as our “measures,” which in our case is our three tests (if unspecified, it just uses all non-id variables, so we could have left it out here):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;traits_long_m &amp;lt;- melt(traits, id.vars=&amp;quot;id&amp;quot;, measure.vars=c(&amp;#39;control&amp;#39;, &amp;#39;treat1&amp;#39;, &amp;#39;treat2&amp;#39;), 
                    variable.name=&amp;#39;test&amp;#39;, value.name=&amp;#39;score&amp;#39;)
print(traits_long_m)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    id    test score
## 1   1 control    36
## 2   2 control    33
## 3   3 control    27
## 4   4 control    26
## 5   5 control    23
## 6   6 control    33
## 7   7 control    30
## 8   8 control    29
## 9   9 control    22
## 10 10 control    23
## 11  1  treat1     8
## 12  2  treat1    11
## 13  3  treat1     9
## 14  4  treat1     6
## 15  5  treat1     9
## 16  6  treat1    12
## 17  7  treat1     9
## 18  8  treat1    13
## 19  9  treat1    12
## 20 10  treat1    10
## 21  1  treat2    21
## 22  2  treat2     9
## 23  3  treat2    13
## 24  4  treat2    14
## 25  5  treat2     8
## 26  6  treat2    13
## 27  7  treat2    19
## 28  8  treat2    11
## 29  9  treat2    11
## 30 10  treat2    16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And let’s not leave out &lt;code&gt;tidyr&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyr)
traits_long_t &amp;lt;- gather(traits, key=test, value=score, control, treat1, treat2)
print(traits_long_t)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    id    test score
## 1   1 control    36
## 2   2 control    33
## 3   3 control    27
## 4   4 control    26
## 5   5 control    23
## 6   6 control    33
## 7   7 control    30
## 8   8 control    29
## 9   9 control    22
## 10 10 control    23
## 11  1  treat1     8
## 12  2  treat1    11
## 13  3  treat1     9
## 14  4  treat1     6
## 15  5  treat1     9
## 16  6  treat1    12
## 17  7  treat1     9
## 18  8  treat1    13
## 19  9  treat1    12
## 20 10  treat1    10
## 21  1  treat2    21
## 22  2  treat2     9
## 23  3  treat2    13
## 24  4  treat2    14
## 25  5  treat2     8
## 26  6  treat2    13
## 27  7  treat2    19
## 28  8  treat2    11
## 29  9  treat2    11
## 30 10  treat2    16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, the key/value pairing tells us about our outcome columns, and then we just list the columns to gather up.&lt;/p&gt;
&lt;p&gt;Three roads, same destination. I find &lt;code&gt;melt&lt;/code&gt; and &lt;code&gt;gather&lt;/code&gt; both much more intuitive than &lt;code&gt;reshape&lt;/code&gt;, with &lt;code&gt;gather&lt;/code&gt; the easiest of them all to use, but your mileage may vary.&lt;/p&gt;
&lt;p&gt;Data is &lt;em&gt;really&lt;/em&gt; easy to plot this way:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
plot &amp;lt;- ggplot(traits_long_t, aes(x=test, y=score, color=test)) +
        geom_point()
print(plot)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../&#34;katherinemwood.github.io/&#34;  # End your URL with a `/` trailing slash.post/wrangling_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt; Simplicity itself.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;long-to-wide&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Long to Wide&lt;/h3&gt;
&lt;p&gt;Now, if we want to go the other direction (long to wide), in base R, we call the same function with different arguments:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;traits_wide_base &amp;lt;- reshape(traits_long_base, direction=&amp;#39;wide&amp;#39;, timevar=&amp;#39;test&amp;#39;, idvar=&amp;#39;id&amp;#39;)
print(traits_wide_base)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            id score.control score.treat1 score.treat2
## 1.control   1            36            8           21
## 2.control   2            33           11            9
## 3.control   3            27            9           13
## 4.control   4            26            6           14
## 5.control   5            23            9            8
## 6.control   6            33           12           13
## 7.control   7            30            9           19
## 8.control   8            29           13           11
## 9.control   9            22           12           11
## 10.control 10            23           10           16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have the original structure of our data back.&lt;/p&gt;
&lt;p&gt;The inverse of &lt;code&gt;melt()&lt;/code&gt; is &lt;code&gt;dcast&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;traits_wide_m &amp;lt;- dcast(traits_long_m, id ~ test, value.var=&amp;#39;score&amp;#39;)
print(traits_wide_m)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    id control treat1 treat2
## 1   1      36      8     21
## 2   2      33     11      9
## 3   3      27      9     13
## 4   4      26      6     14
## 5   5      23      9      8
## 6   6      33     12     13
## 7   7      30      9     19
## 8   8      29     13     11
## 9   9      22     12     11
## 10 10      23     10     16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Right back to where we were.&lt;/p&gt;
&lt;p&gt;And to undo &lt;code&gt;gather&lt;/code&gt;, we &lt;code&gt;spread&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;traits_wide_t &amp;lt;- spread(traits_long_t, test, score)
print(traits_wide_t)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    id control treat1 treat2
## 1   1      36      8     21
## 2   2      33     11      9
## 3   3      27      9     13
## 4   4      26      6     14
## 5   5      23      9      8
## 6   6      33     12     13
## 7   7      30      9     19
## 8   8      29     13     11
## 9   9      22     12     11
## 10 10      23     10     16&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;reshaping-with-more-variables&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Reshaping with more variables&lt;/h3&gt;
&lt;p&gt;Here’s a more complex example.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;traittest &amp;lt;- data.frame(&amp;#39;traitA&amp;#39;=factor(rep(c(&amp;#39;high&amp;#39;, &amp;#39;med&amp;#39;, &amp;#39;low&amp;#39;), each=4)),
                        &amp;#39;traitB&amp;#39;=factor(rep(c(&amp;#39;positive&amp;#39;, &amp;#39;negative&amp;#39;), times=6)),
                        &amp;#39;test1&amp;#39;=floor(rnorm(12, 10, 2)), &amp;#39;test2&amp;#39;=floor(rnorm(12, 15, 2)))
head(traittest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   traitA   traitB test1 test2
## 1   high positive     9    14
## 2   high negative     7    16
## 3   high positive    11    18
## 4   high negative    12    16
## 5    med positive     8    16
## 6    med negative     8    16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are a lot of ways to melt this data. Maybe we want to collpase the tests into a single column–in this case the traits are the identifier variables.&lt;/p&gt;
&lt;p&gt;In base:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tt_bytrait_base &amp;lt;- reshape(traittest, direction=&amp;#39;long&amp;#39;, v.names=&amp;#39;score&amp;#39;,
                           timevar=&amp;#39;test&amp;#39;, times=c(&amp;#39;test1&amp;#39;, &amp;#39;test2&amp;#39;), varying=c(&amp;#39;test1&amp;#39;,&amp;#39;test2&amp;#39;))
print(tt_bytrait_base)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          traitA   traitB  test score id
## 1.test1    high positive test1     9  1
## 2.test1    high negative test1     7  2
## 3.test1    high positive test1    11  3
## 4.test1    high negative test1    12  4
## 5.test1     med positive test1     8  5
## 6.test1     med negative test1     8  6
## 7.test1     med positive test1    10  7
## 8.test1     med negative test1     9  8
## 9.test1     low positive test1     6  9
## 10.test1    low negative test1    11 10
## 11.test1    low positive test1    13 11
## 12.test1    low negative test1     9 12
## 1.test2    high positive test2    14  1
## 2.test2    high negative test2    16  2
## 3.test2    high positive test2    18  3
## 4.test2    high negative test2    16  4
## 5.test2     med positive test2    16  5
## 6.test2     med negative test2    16  6
## 7.test2     med positive test2    16  7
## 8.test2     med negative test2    11  8
## 9.test2     low positive test2    17  9
## 10.test2    low negative test2    13 10
## 11.test2    low positive test2    18 11
## 12.test2    low negative test2    12 12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With &lt;code&gt;melt()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tt_bytrait_m &amp;lt;- melt(traittest, measure.vars=c(&amp;#39;test1&amp;#39;, &amp;#39;test2&amp;#39;), variable.name=&amp;#39;test&amp;#39;,
                  value.name=&amp;#39;score&amp;#39;)
head(tt_bytrait_m)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   traitA   traitB  test score
## 1   high positive test1     9
## 2   high negative test1     7
## 3   high positive test1    11
## 4   high negative test1    12
## 5    med positive test1     8
## 6    med negative test1     8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With &lt;code&gt;gather&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tt_bytrait_t &amp;lt;- gather(traittest, test, score, test1, test2)
head(tt_bytrait_t)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   traitA   traitB  test score
## 1   high positive test1     9
## 2   high negative test1     7
## 3   high positive test1    11
## 4   high negative test1    12
## 5    med positive test1     8
## 6    med negative test1     8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or, we can let the tests be the identifiers, and collapse the traits into a single column.&lt;/p&gt;
&lt;p&gt;Base:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tt_bytest_base &amp;lt;- reshape(traittest, direction=&amp;#39;long&amp;#39;, v.names=&amp;#39;rating&amp;#39;,
                          timevar=&amp;#39;trait&amp;#39;, times=c(&amp;#39;traitA&amp;#39;, &amp;#39;traitB&amp;#39;), 
                          varying=c(&amp;#39;traitA&amp;#39;,&amp;#39;traitB&amp;#39;))
print(tt_bytest_base)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           test1 test2  trait   rating id
## 1.traitA      9    14 traitA     high  1
## 2.traitA      7    16 traitA     high  2
## 3.traitA     11    18 traitA     high  3
## 4.traitA     12    16 traitA     high  4
## 5.traitA      8    16 traitA      med  5
## 6.traitA      8    16 traitA      med  6
## 7.traitA     10    16 traitA      med  7
## 8.traitA      9    11 traitA      med  8
## 9.traitA      6    17 traitA      low  9
## 10.traitA    11    13 traitA      low 10
## 11.traitA    13    18 traitA      low 11
## 12.traitA     9    12 traitA      low 12
## 1.traitB      9    14 traitB positive  1
## 2.traitB      7    16 traitB negative  2
## 3.traitB     11    18 traitB positive  3
## 4.traitB     12    16 traitB negative  4
## 5.traitB      8    16 traitB positive  5
## 6.traitB      8    16 traitB negative  6
## 7.traitB     10    16 traitB positive  7
## 8.traitB      9    11 traitB negative  8
## 9.traitB      6    17 traitB positive  9
## 10.traitB    11    13 traitB negative 10
## 11.traitB    13    18 traitB positive 11
## 12.traitB     9    12 traitB negative 12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;melt&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tt_bytest_m &amp;lt;- melt(traittest, measure.vars=c(&amp;#39;traitA&amp;#39;, &amp;#39;traitB&amp;#39;),
                  variable.name=&amp;#39;trait&amp;#39;, value.name=&amp;#39;rating&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: attributes are not identical across measure variables; they will
## be dropped&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(tt_bytest_m)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   test1 test2  trait rating
## 1     9    14 traitA   high
## 2     7    16 traitA   high
## 3    11    18 traitA   high
## 4    12    16 traitA   high
## 5     8    16 traitA    med
## 6     8    16 traitA    med&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(We can ignore the warning; it’s warning us about the fact that we’re combining two factors that don’t share levels, so it’s coercing them all to characters.)&lt;/p&gt;
&lt;p&gt;With &lt;code&gt;gather&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tt_bytest_t &amp;lt;- gather(traittest, trait, rating, traitA, traitB)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: attributes are not identical across measure variables; they will
## be dropped&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(tt_bytest_t)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   test1 test2  trait rating
## 1     9    14 traitA   high
## 2     7    16 traitA   high
## 3    11    18 traitA   high
## 4    12    16 traitA   high
## 5     8    16 traitA    med
## 6     8    16 traitA    med&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(Same warning as above.)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;reformatting-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Reformatting Data&lt;/h1&gt;
&lt;p&gt;So we’ve read data in, and can flip it between long and wide at will. Great, but what if the data itself needs to be fixed?&lt;/p&gt;
&lt;div id=&#34;recoding-values&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Recoding values&lt;/h3&gt;
&lt;p&gt;Let’s say you have some data that look like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;yesno &amp;lt;- data.frame(&amp;#39;subj&amp;#39;=seq(1,10), &amp;#39;resp&amp;#39;=rep(c(&amp;#39;Y&amp;#39;,&amp;#39;N&amp;#39;), each=5))
print(yesno)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    subj resp
## 1     1    Y
## 2     2    Y
## 3     3    Y
## 4     4    Y
## 5     5    Y
## 6     6    N
## 7     7    N
## 8     8    N
## 9     9    N
## 10   10    N&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we have 10 subjects, and each one responded either yes (Y) or no (N) to… something. But maybe we don’t like the way this is coded; Y and N are hard to work with if we want to find average accuracy, for example. Maybe we want 1’s and 0’s instead, with which it is easy to do calculations.&lt;/p&gt;
&lt;p&gt;If we want to recode these values, we have a few options. We can use indexing, of course, but there are also some functions that will save you some work.&lt;/p&gt;
&lt;p&gt;Base has the &lt;code&gt;ifelse&lt;/code&gt; function, which performs a logical comparison, and if true, returns the first value; else, the second:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;yesno$resp &amp;lt;- ifelse(yesno$resp == &amp;#39;Y&amp;#39;, 1, 0)
print(yesno)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    subj resp
## 1     1    1
## 2     2    1
## 3     3    1
## 4     4    1
## 5     5    1
## 6     6    0
## 7     7    0
## 8     8    0
## 9     9    0
## 10   10    0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we have more than two alternatives, you’ll have to use something like a &lt;code&gt;switch&lt;/code&gt; statement:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;yesnomaybe &amp;lt;- data.frame(&amp;#39;subj&amp;#39;=seq(1,15), &amp;#39;resp&amp;#39;=rep(c(&amp;#39;Y&amp;#39;,&amp;#39;N&amp;#39;,&amp;#39;M&amp;#39;), each=5))
print(yesnomaybe)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    subj resp
## 1     1    Y
## 2     2    Y
## 3     3    Y
## 4     4    Y
## 5     5    Y
## 6     6    N
## 7     7    N
## 8     8    N
## 9     9    N
## 10   10    N
## 11   11    M
## 12   12    M
## 13   13    M
## 14   14    M
## 15   15    M&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have three options. Maybe we want ‘yes’ to be 1, ‘no’ to be -1, and ‘maybe’ to be 0. Here’s how you can do it with a &lt;code&gt;switch&lt;/code&gt; statement and &lt;code&gt;sapply&lt;/code&gt; to call it on each element:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;yesnomaybe$resp &amp;lt;- sapply(yesnomaybe$resp, function(x) switch(as.character(x), &amp;#39;Y&amp;#39;=1, &amp;#39;N&amp;#39;=-1,
                                                              &amp;#39;M&amp;#39;=0))
print(yesnomaybe)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    subj resp
## 1     1    1
## 2     2    1
## 3     3    1
## 4     4    1
## 5     5    1
## 6     6   -1
## 7     7   -1
## 8     8   -1
## 9     9   -1
## 10   10   -1
## 11   11    0
## 12   12    0
## 13   13    0
## 14   14    0
## 15   15    0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In &lt;code&gt;dplyr&lt;/code&gt;, we have the &lt;code&gt;recode&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;yesnomaybe$dplyr_recode &amp;lt;- recode(yesnomaybe$resp, `1`=&amp;#39;yes&amp;#39;, `-1`=&amp;#39;no&amp;#39;, `0`=&amp;#39;maybe&amp;#39;)
print(yesnomaybe)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    subj resp dplyr_recode
## 1     1    1          yes
## 2     2    1          yes
## 3     3    1          yes
## 4     4    1          yes
## 5     5    1          yes
## 6     6   -1           no
## 7     7   -1           no
## 8     8   -1           no
## 9     9   -1           no
## 10   10   -1           no
## 11   11    0        maybe
## 12   12    0        maybe
## 13   13    0        maybe
## 14   14    0        maybe
## 15   15    0        maybe&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Recoding, assuming you don’t have to do it for a huge number of possibilities, goes pretty fast.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-variables&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Adding variables&lt;/h3&gt;
&lt;p&gt;Variables can be added to an existing data frame just with the &lt;code&gt;$&lt;/code&gt; operator:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- data.frame(&amp;#39;x&amp;#39;=rnorm(20, 6), &amp;#39;y&amp;#39;=rnorm(20))
print(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           x           y
## 1  6.182055  0.34012358
## 2  7.436347 -0.59966661
## 3  6.315306  0.36833177
## 4  6.896922  1.50183433
## 5  6.941559  2.49651251
## 6  5.310434 -0.03238655
## 7  5.571108 -1.14902213
## 8  6.196074  0.84410316
## 9  6.070575 -0.98276613
## 10 7.307604 -0.41943093
## 11 6.639640  0.85786488
## 12 6.036730  0.59530225
## 13 5.319429  0.51228832
## 14 4.451193 -0.78561172
## 15 5.352494  0.87007150
## 16 8.215930 -0.67282494
## 17 5.485161 -0.17377787
## 18 6.126534 -0.03902882
## 19 7.063122 -0.57183073
## 20 5.599426  0.61108155&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df$z &amp;lt;- rnorm(20, 10)
print(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           x           y         z
## 1  6.182055  0.34012358 10.298640
## 2  7.436347 -0.59966661  9.799344
## 3  6.315306  0.36833177  9.668752
## 4  6.896922  1.50183433 10.097066
## 5  6.941559  2.49651251 10.815749
## 6  5.310434 -0.03238655  9.597003
## 7  5.571108 -1.14902213 11.245294
## 8  6.196074  0.84410316  8.256783
## 9  6.070575 -0.98276613 10.099128
## 10 7.307604 -0.41943093 11.489885
## 11 6.639640  0.85786488  9.110691
## 12 6.036730  0.59530225  8.862010
## 13 5.319429  0.51228832 10.119818
## 14 4.451193 -0.78561172 10.767057
## 15 5.352494  0.87007150  9.525280
## 16 8.215930 -0.67282494  9.805977
## 17 5.485161 -0.17377787 10.306639
## 18 6.126534 -0.03902882  9.573121
## 19 7.063122 -0.57183073  9.948351
## 20 5.599426  0.61108155  9.301278&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you need to manipulate two data vectors that are numeric, you can just add, multiply, etc. your columns together to perform these operations elementwise:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df$total &amp;lt;- with(df, x + y + z)
head(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          x           y         z    total
## 1 6.182055  0.34012358 10.298640 16.82082
## 2 7.436347 -0.59966661  9.799344 16.63602
## 3 6.315306  0.36833177  9.668752 16.35239
## 4 6.896922  1.50183433 10.097066 18.49582
## 5 6.941559  2.49651251 10.815749 20.25382
## 6 5.310434 -0.03238655  9.597003 14.87505&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You also have a lot of options in the &lt;code&gt;dplyr&lt;/code&gt; library, notably &lt;code&gt;transform&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- transform(df, x = -x)
head(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           x           y         z    total
## 1 -6.182055  0.34012358 10.298640 16.82082
## 2 -7.436347 -0.59966661  9.799344 16.63602
## 3 -6.315306  0.36833177  9.668752 16.35239
## 4 -6.896922  1.50183433 10.097066 18.49582
## 5 -6.941559  2.49651251 10.815749 20.25382
## 6 -5.310434 -0.03238655  9.597003 14.87505&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But now that we’ve updated a column, our total is wrong. Let’s fix it with &lt;code&gt;transmute&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- mutate(df, corrected_total = x + y + z)
head(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           x           y         z    total corrected_total
## 1 -6.182055  0.34012358 10.298640 16.82082        4.456709
## 2 -7.436347 -0.59966661  9.799344 16.63602        1.763331
## 3 -6.315306  0.36833177  9.668752 16.35239        3.721778
## 4 -6.896922  1.50183433 10.097066 18.49582        4.701978
## 5 -6.941559  2.49651251 10.815749 20.25382        6.370702
## 6 -5.310434 -0.03238655  9.597003 14.87505        4.254183&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Maybe I now want a dataframe just of the even numbers in the x column, and the residuals from total and corrected total (for… reasons). &lt;code&gt;transmute&lt;/code&gt; is like mutate, but it throws away all the extra:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_even &amp;lt;- transmute(df, x_ev=floor(x)%%2==0, residuals=total-corrected_total)
head(df_even)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    x_ev residuals
## 1 FALSE  12.36411
## 2  TRUE  14.87269
## 3 FALSE  12.63061
## 4 FALSE  13.79384
## 5 FALSE  13.88312
## 6  TRUE  10.62087&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If none of these methods fit the bill, you can call &lt;code&gt;apply&lt;/code&gt; along all the columns or rows of your data frame and write a custom function to do whatever processing you need.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;factor-levels-as-column-labels&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Factor levels as column labels&lt;/h3&gt;
&lt;p&gt;Let’s take the unfortunate case of levels-as-columns, in which all the levels of a factor are columns, and people get a 1 or a 0 for each level instead of their value. Here’s some example data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;levs &amp;lt;- data.frame(&amp;#39;subj&amp;#39;=seq(1, 4), &amp;#39;a&amp;#39;=c(0, 0, 1, 0), &amp;#39;b&amp;#39;=c(1, 0, 0, 1), &amp;#39;c&amp;#39;=c(0, 1, 0, 0))
print(levs)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   subj a b c
## 1    1 0 1 0
## 2    2 0 0 1
## 3    3 1 0 0
## 4    4 0 1 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So, what we have are three subjects, and a factor with three possible levels: A, B, and C. What we want is the subject and the actual level of their factor, so we need a 2-column matrix.&lt;/p&gt;
&lt;p&gt;Here’s one way we might do that (there are others) that uses some procedures I’ve already shown. First, we’ll reshape the dataframe so that the factors end up in one column. This has the advantage of putting the actual values of the factors we want all in one place. Then we filter out the 0s, leaving behind only the levels the subject actually selected, drop the redundant ones colum, then put the subjects back in the right order.&lt;/p&gt;
&lt;p&gt;For these examples, I’ll print out each intermediate stage of manipulation so that you can see what’s happening.&lt;/p&gt;
&lt;p&gt;All about that base:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(lev_long &amp;lt;- reshape(levs, idvar=&amp;#39;subj&amp;#39;, direction=&amp;#39;long&amp;#39;, v.names=&amp;#39;value&amp;#39;, timevar=&amp;#39;trait&amp;#39;,
                    times=c(&amp;#39;a&amp;#39;, &amp;#39;b&amp;#39;, &amp;#39;c&amp;#39;), varying=c(&amp;#39;a&amp;#39;, &amp;#39;b&amp;#39;, &amp;#39;c&amp;#39;)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     subj trait value
## 1.a    1     a     0
## 2.a    2     a     0
## 3.a    3     a     1
## 4.a    4     a     0
## 1.b    1     b     1
## 2.b    2     b     0
## 3.b    3     b     0
## 4.b    4     b     1
## 1.c    1     c     0
## 2.c    2     c     1
## 3.c    3     c     0
## 4.c    4     c     0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First, we reshape the data. We need all of the factor-related pieces of information in a single column. We have a column with the possible factor levels, and a column indicating 0 (not the subject’s level) or 1 (the subject’s level).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(lev_filtered &amp;lt;- with(lev_long, lev_long[value == 1, 1:2]))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     subj trait
## 3.a    3     a
## 1.b    1     b
## 4.b    4     b
## 2.c    2     c&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second step just uses good old-fashioned indexing to keep all rows where the value is 1 (aka, the subject has that level), and to keep only the useful subject and trait columns; what the &lt;code&gt;with&lt;/code&gt; function does is tell R to perform all operations with the supplied data set, so we can reference columns by isolated names rather than having to do the verbose &lt;code&gt;data_frame$column&lt;/code&gt; syntax.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(lev_reformed_base &amp;lt;- lev_filtered[order(lev_filtered$subj),])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     subj trait
## 1.b    1     b
## 2.c    2     c
## 3.a    3     a
## 4.b    4     b&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The final step is reordering the data according to the subject column in ascending order. Now we’ve got our data in a much more sensible format.&lt;/p&gt;
&lt;p&gt;Tidyr and dplyr make quick work of this. First, we gather:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(lev_g &amp;lt;- gather(levs, trait, value, a, b, c))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    subj trait value
## 1     1     a     0
## 2     2     a     0
## 3     3     a     1
## 4     4     a     0
## 5     1     b     1
## 6     2     b     0
## 7     3     b     0
## 8     4     b     1
## 9     1     c     0
## 10    2     c     1
## 11    3     c     0
## 12    4     c     0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Filter out the 0s:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(lev_f &amp;lt;- filter(lev_g, value != 0))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   subj trait value
## 1    3     a     1
## 2    1     b     1
## 3    4     b     1
## 4    2     c     1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Retain only the useful columns:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(lev_s &amp;lt;- select(lev_f, subj, trait))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   subj trait
## 1    3     a
## 2    1     b
## 3    4     b
## 4    2     c&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, put the subjects back in order:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(lev_reform &amp;lt;- arrange(lev_s, subj))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   subj trait
## 1    1     b
## 2    2     c
## 3    3     a
## 4    4     b&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here are those steps strung together with piping and thus obviating the need for all those separate variable assignments:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;levs_reformed &amp;lt;- gather(levs, trait, value, a, b, c) %&amp;gt;%
                filter(value != 0) %&amp;gt;%
                select(subj, trait) %&amp;gt;%
                arrange(subj)
print(levs_reformed)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   subj trait
## 1    1     b
## 2    2     c
## 3    3     a
## 4    4     b&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Charming!&lt;/p&gt;
&lt;p&gt;What about if we have multiple factors? Here we have a test and a report, each of which has three possible levels: ABC and XYZ, respectively.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mfac &amp;lt;- data.frame(&amp;#39;subj&amp;#39;=seq(1, 4), &amp;#39;test.A&amp;#39;=c(0, 1, 0, 1), &amp;#39;test.B&amp;#39;=c(1, 0, 0, 0), 
                   &amp;#39;test.C&amp;#39;=c(0, 0, 1, 0), &amp;#39;report.X&amp;#39;=c(1, 0, 0, 0), 
                   &amp;#39;report.Y&amp;#39;=c(0, 1, 1, 0), &amp;#39;report.Z&amp;#39;=c(0, 0, 0, 1))
print(mfac)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   subj test.A test.B test.C report.X report.Y report.Z
## 1    1      0      1      0        1        0        0
## 2    2      1      0      0        0        1        0
## 3    3      0      0      1        0        1        0
## 4    4      1      0      0        0        0        1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So what we want is a dataframe with three columns: subject number, test, and report. Subject 1 picked test A and report X, subject 2 picked test A and report Y, and so on.&lt;/p&gt;
&lt;p&gt;This gets a little more complicated. If we collapse everything into one column, we’re going to have to then spread it back out to separate the factors. We’ve also got the item label merged to its type, which is a problem if we only want the letter designation.&lt;/p&gt;
&lt;p&gt;Let’s try with base. Here’s the reshape-filter method:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mfac_long &amp;lt;- reshape(mfac, idvar=&amp;#39;subj&amp;#39;, direction=&amp;#39;long&amp;#39;, v.names=&amp;#39;value&amp;#39;, timevar=&amp;#39;measure&amp;#39;,
                    times=colnames(mfac)[-1], varying=colnames(mfac)[-1])
mfac_filtered &amp;lt;- with(mfac_long, mfac_long[value == 1, 1:2])
type_splits &amp;lt;- do.call(rbind, strsplit(mfac_filtered$measure, &amp;#39;.&amp;#39;, fixed=TRUE))
mfac_sep &amp;lt;- data.frame(&amp;#39;subj&amp;#39;=mfac_filtered$subj, 
                       &amp;#39;type&amp;#39;=type_splits[,1], 
                       &amp;#39;version&amp;#39;=type_splits[,2])
mfac_wide &amp;lt;- reshape(mfac_sep, idvar=&amp;#39;subj&amp;#39;, direction=&amp;#39;wide&amp;#39;, timevar=&amp;#39;type&amp;#39;)
(mfac_reformed_base &amp;lt;- mfac_wide[order(mfac_wide$subj),])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   subj version.test version.report
## 3    1            B              X
## 1    2            A              Y
## 4    3            C              Y
## 2    4            A              Z&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Pulling this off takes more finagling. Things are fine when we reshape and filter (note the trick used to save some verbage in reshape(); indexing with a negative excludes that item, so we’re saying we want all column names except the first), but then we have to recover whether our factor was a test or a report &lt;em&gt;separately&lt;/em&gt; of its type. This means we have to split the string using &lt;code&gt;strsplit&lt;/code&gt;, bind the results into a matrix (because they automatically come out as a list), and then take those newly-made factors and reshape it wide again with the test type and report type as their own columns. One nice thing about this approach, in spite of its many steps, is that it’s totally blind to the content of the labels (provided they are consistently delimited). If they’re labeled in a cooperative way, you don’t need to know how many labels there are or what they say, and they can be in any order.&lt;/p&gt;
&lt;p&gt;Here’s another base approach, from my BFF Kelly Chang. This one uses the &lt;code&gt;apply&lt;/code&gt; function to sweep a filter down the dataframe, then repackage the results:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;labels &amp;lt;- c(&amp;#39;A&amp;#39;, &amp;#39;B&amp;#39;, &amp;#39;C&amp;#39;, &amp;#39;X&amp;#39;, &amp;#39;Y&amp;#39;, &amp;#39;Z&amp;#39;)
filtered &amp;lt;- t(apply(mfac[,2:ncol(mfac)], 1, function(x) labels[x==1]))
mfac_kc &amp;lt;- data.frame(mfac$subj, filtered)
colnames(mfac_kc) &amp;lt;- c(&amp;#39;subj&amp;#39;, &amp;#39;test&amp;#39;, &amp;#39;report&amp;#39;)
print(mfac_kc)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   subj test report
## 1    1    B      X
## 2    2    A      Y
## 3    3    C      Y
## 4    4    A      Z&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, you would supply the labels, rather than recovering them from the data itself (as was done in the previous approach). Here, order is important; the labels need to be in the same order as the corresponding columns for the filter to work.&lt;/p&gt;
&lt;p&gt;With &lt;code&gt;tidyr&lt;/code&gt; and &lt;code&gt;dplyr&lt;/code&gt;, this approach can look something like this (still agnostic to the label content):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mfac_reformed &amp;lt;- gather(mfac, measure, value, -subj) %&amp;gt;%
                filter(value != 0) %&amp;gt;%
                select(subj, measure) %&amp;gt;%
                separate(measure, c(&amp;#39;test&amp;#39;, &amp;#39;type&amp;#39;)) %&amp;gt;%
                spread(test, type) %&amp;gt;%
                arrange(subj)
print(mfac_reformed)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   subj report test
## 1    1      X    B
## 2    2      Y    A
## 3    3      Y    C
## 4    4      Z    A&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first few steps are the same; melt everything down and toss the zero values. Then, we need a step to yank apart the measure’s letter designation and its type. Fortunately, &lt;code&gt;tidyr&lt;/code&gt; has a handy &lt;code&gt;separate&lt;/code&gt; function that does just this; it pulls apart the joined values into two columns that we can label right away. Then, we need to spread our now distinct factor types back into columns–one for the test and one for the report–and sort by subject.&lt;/p&gt;
&lt;p&gt;Note also that the intermediate steps in this last example, when we had to separate the two types of factors and get two separate ones back from the &lt;code&gt;report.X&lt;/code&gt; format, which involved splitting the string and reshaping the data, can also be useful if you have data in this form, or if you have one big code for a condition or trial and at some point want to split it into its components. You can also use the &lt;code&gt;colsplit()&lt;/code&gt; function from the &lt;code&gt;reshape2&lt;/code&gt; package for this purpose.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;parting-thoughts&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Parting Thoughts&lt;/h1&gt;
&lt;p&gt;And there you have it–a brief introduction to some common data manipulation tasks, and a few ways to handle them. This is only the thinnest of samples of methods. There are lots of different ways to accomplish things, and packages to help you do it. Many of these methods will undoubtedly have my fingerprints all over them; one of the reasons I approached these problems the way I did is to show how learning a skill in one context–reshaping data for plotting, for example–can be useful in other contexts, like changing a data frame’s fundamental structure. Many roads lead to the same place, and if you don’t like this one, another will get you there just as comfortably, if not more so.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Intro to Bootstrapping</title>
      <link>https://katherinemwood.github.io/post/bootstrapping/</link>
      <pubDate>Mon, 26 Jun 2017 15:02:58 -0500</pubDate>
      
      <guid>https://katherinemwood.github.io/post/bootstrapping/</guid>
      <description>&lt;p&gt;The bootstrap, detailed by Efron and Tibshirani in their 1993 book &lt;em&gt;An Introduction to the Bootstrap,&lt;/em&gt; is a powerful, flexible statistical concept based on the idea of “pulling yourself up by your bootstraps.” With today’s computing power, it’s easier than ever to apply and use. It also happens to be one of my favorite statistical tricks because of the elegance of the underlying logic.&lt;/p&gt;
&lt;p&gt;The idea behind the bootstrap is simple. We know that if we repeatedly sample groups from the population, our measurement of that population will get increasingly accurate, becoming perfect when we’ve sampled every member of the population (and thus obviating the need for statistics at all). However, this world, like worlds without friction in physics, don’t resemble operating conditions. In the real world, you typically get one sample from your population. If only we could easily resample from the population a few more times.&lt;/p&gt;
&lt;p&gt;Bootstrapping gets you the next best thing. We don’t resample from the population. Instead, we continuously resample &lt;em&gt;our own data&lt;/em&gt;,with replacement, and generate a so-called bootstrapped distribution. We can then use this distribution to quantify uncertainty on all kinds of measures.&lt;/p&gt;
&lt;p&gt;I’ll work through an example to show how this works in practice. We’ll sample from the normal to start, &lt;span class=&#34;math inline&#34;&gt;\(\mu = 1.25\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\sigma = 0.5\)&lt;/span&gt;. I’ve set the seed within the script so you’ll get exactly the results I find here.&lt;/p&gt;
&lt;p&gt;Let’s say we run a study with 50 people, and we’re interested in, among other things, getting a good estimation of the mean of the underlying population. So, you take your 50 people, take a mean, get a standard error, and maybe 95% confidence intervals to show the variation (± 1.96*SE).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(effsize)
#Set the seed for reproducable results
set.seed(123)
#Generate some random normal data
x &amp;lt;- rnorm(50, 1.25, .5)
mean(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.267202&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.462935&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Calculate the SE the old fashioned way
se &amp;lt;- sd(x)/sqrt(50)
#Get 95% CIs with the formula
lb_se &amp;lt;- mean(x) - 1.96*se
ub_se &amp;lt;- mean(x) + 1.96*se&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;[Just to note: bootstrapped confidence intervals give you a nice measure of uncertainty, and I like to use them as my error bars, but they don’t get you away from the problems of NHST, if that’s something you’re trying to avoid.]&lt;/p&gt;
&lt;p&gt;Here’s a histogram of our sample. In this post, I’m plotting everything in base just for the thrill:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(x,  breaks=10)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../&#34;katherinemwood.github.io/&#34;  # End your URL with a `/` trailing slash.post/bootstrapping_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here are the stats for our mean, calculated the old-fashioned way from an n of 50:&lt;/p&gt;
&lt;p&gt;mean: 1.27&lt;br /&gt;
SE: 0.18&lt;br /&gt;
95% CI: [1.14, 1.40]&lt;/p&gt;
&lt;p&gt;Now, let’s bootstrap this mean. A way to conceptualize this is to imagine re-running your 50-person experiment over and over again, except we’re not going to be drawing new subjects from the population. Instead, we’re going to draw groups of 50 subjects from our &lt;em&gt;sample,&lt;/em&gt; but do so with replacement. This means that some subjects may show up more than once, and some may never show up at all. We’ll take the mean of each group. We’ll do this many times–here, 1000–and so we end up with a distribution of 1000 means.&lt;/p&gt;
&lt;p&gt;For this basic procedure, we can do it in just one line:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;boot &amp;lt;- replicate(1000, mean(sample(x, 50, replace=TRUE)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And here’s the resulting histogram:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lb &amp;lt;- sort(boot)[.025*1000]
ub &amp;lt;- sort(boot)[.975*1000]
#Plot a histogram of the bootstrapped distribution, the
#mean, and the 95% bootstrapped CI
hist(boot, breaks=50)
abline(v=c(mean(boot), lb, ub), col=rep(&amp;#39;red&amp;#39;, 3), lty=c(1, 2, 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../&#34;katherinemwood.github.io/&#34;  # End your URL with a `/` trailing slash.post/bootstrapping_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The bootstrapped mean (1.27) is of course centered on our sample mean. Now we have a distribution of values for the mean that we could expect to obtain with our sample.&lt;/p&gt;
&lt;p&gt;Now, if we wanted to do confidence intervals, we could get the SE of the bootstrapped distribution and do it the usual way. However, we also have an option available to us called the &lt;em&gt;percentile method.&lt;/em&gt; In order to find the confidence intervals, we sort all of our bootstrapped values, and then take the 2.5% quantile and the 97.5% quantile. If you repeat the sampling procedure with the same process on the same population (precisely what we do with our bootstrap), 95% of the time the mean falls between these endpoints. If we do this procedure with our particular sample, we get this:&lt;/p&gt;
&lt;p&gt;95% bootstrapped CI: [1.14, 1.40]&lt;/p&gt;
&lt;p&gt;Here, the bootstrapped CI and the standard-error derived CI match up (not that surprising, since our data is normally distributed and therefore exceptionally well-behaved!). However, if you have constrained data, such as accuracy, bootstrapped confidence intervals will automatically conform to those limits and won’t exceed 100% or dip below 0%.&lt;/p&gt;
&lt;p&gt;Here’s an example of bootstrapping the estimate of the effect size. We’re going to assume a between-subjects design here. We’ll make two independent, normally distributed groups with a true effect of .5. I’m going to let the &lt;code&gt;cohen.d&lt;/code&gt; function from the effsize package do the heavy lifting and give me both the estimate and the confidence interval:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;groups &amp;lt;- data.frame(&amp;#39;exp&amp;#39;=rnorm(50, .5, 1), &amp;#39;control&amp;#39;=rnorm(50))
(es &amp;lt;- cohen.d(groups$exp, groups$control))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Cohen&amp;#39;s d
## 
## d estimate: 0.5537571 (medium)
## 95 percent confidence interval:
##       inf       sup 
## 0.1493285 0.9581857&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s how we would bootstrap those same intervals. We resample each group independently, because it’s between-subjects. The procedure is much the same if it’s within subjects, except you would resample your &lt;em&gt;pairs&lt;/em&gt; of data because the two groups are not independent in that case. The code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;boot_es &amp;lt;- replicate(1000, cohen.d(sample(groups$exp, 50, replace=TRUE), 
                                   sample(groups$control, 50, replace=TRUE))$estimate)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And here’s the histogram of the bootstrapped distribution, with the mean and bootstrapped CI plotted:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lb_es &amp;lt;- sort(boot_es)[.025*1000]
ub_es &amp;lt;- sort(boot_es)[.975*1000]
hist(boot_es, breaks=50)
abline(v=c(mean(boot_es), lb_es, ub_es), col=rep(&amp;#39;red&amp;#39;, 3), lty=c(1, 2, 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../&#34;katherinemwood.github.io/&#34;  # End your URL with a `/` trailing slash.post/bootstrapping_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;bootstrapped mean: .56&lt;br /&gt;
95% bootstrapped CI: [.14, .99]&lt;/p&gt;
&lt;p&gt;And there you have it: bootstrapped confidence intervals on your effect sizes.&lt;/p&gt;
&lt;p&gt;There is an &lt;a href=&#34;https://en.wikipedia.org/wiki/Bootstrapping_(statistics)&#34;&gt;incredible about of nuance&lt;/a&gt; in the bootstrap method, and many different ways to apply it to even the most complex data. You can use it for something as simple as confidence intervals on a mean, or as complicated as regression coefficients. It’s also great for getting CI estimates for statistics or measures that don’t have a formula method. The general idea–resample your own data to get estimates–underlies even the most complex applications of this method. If you understand the basic logic, it’s pretty easy to understand and start using new applications.&lt;/p&gt;
&lt;p&gt;If you are mathematically inclined, there have been many proofs and a lot of work showing the first- and second-order correctness of bootstrapping estimations. The method I showed here for confidence intervals is just the percentile method; although it has been shown to work well in a wide variety of situations, there are other approaches, some which offer bias correction and other improvements (&lt;a href=&#34;http://www-rohan.sdsu.edu/~babailey/stat672/BCa.pdf&#34;&gt;for example&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;If you want to get started, there are some R packages that will handle bootstrapping for you:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/web/packages/boot/boot.pdf&#34;&gt;boot&lt;/a&gt; (general-purpose, and can generate many variants of the various methods)&lt;br /&gt;
&lt;a href=&#34;https://cran.r-project.org/web/packages/bootES/bootES.pdf&#34;&gt;bootES&lt;/a&gt; (for effect sizes in particular)&lt;/p&gt;
&lt;p&gt;And of course, if you want a really deep dive, you can check out the original book:&lt;/p&gt;
&lt;p&gt;Efron &amp;amp; Tibshirani, &lt;em&gt;An Introduction to the Bootstrap.&lt;/em&gt; Chapman &amp;amp; Hall/CRC, 1993.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
